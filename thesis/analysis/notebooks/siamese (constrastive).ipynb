{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Image\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Input, Conv1D, BatchNormalization, Activation, MaxPooling1D, Flatten, Dense, Lambda, Dropout\n",
    "from keras.models import Sequential, Model, load_model, model_from_json\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import itertools\n",
    "import pickle\n",
    "import keras\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BALANCED = False\n",
    "model_dir = os.path.join(os.getcwd(), \"models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, class_labels, model_name, cmap=plt.cm.Blues):\n",
    "    diagram_dir = os.path.join(os.getcwd(), \"images\")\n",
    "    if not os.path.exists(diagram_dir):\n",
    "        os.makedirs(diagram_dir)\n",
    "    \n",
    "    stats_dir = os.path.join(os.getcwd(), \"stats\")\n",
    "    if not os.path.exists(stats_dir):\n",
    "        os.makedirs(stats_dir)\n",
    "    \n",
    "    # create the figure\n",
    "    fig = plt.figure()\n",
    "    fig.set_size_inches(18,15)\n",
    "    \n",
    "    # labels\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    \n",
    "    # tick marks\n",
    "    class_count = len(class_labels)\n",
    "    tick_marks = np.arange(class_count + 1)\n",
    "    plt.xticks(tick_marks, class_labels, rotation=45)\n",
    "    plt.yticks(tick_marks, class_labels)\n",
    "    \n",
    "    # normalization\n",
    "    cm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "    cm = np.around(cm, 6)\n",
    "    \n",
    "    # numbers\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, \"{:.4f}\".format(cm[i, j]), horizontalalignment=\"center\", color=\"white\" if cm[i,j] > thresh else \"black\")\n",
    "    \n",
    "    # color bar\n",
    "    plt.imshow(cm, interpolation=\"nearest\", cmap=cmap)\n",
    "    plt.colorbar()\n",
    "    \n",
    "    # save and show\n",
    "    np.set_printoptions(formatter={\"float\": lambda x: \"{0:0.4f}\".format(x)})\n",
    "    plt.savefig(os.path.join(diagram_dir, model_name + \"_confusion_matrix.png\"), dpi=100)\n",
    "    pd.DataFrame(cm).to_csv(os.path.join(stats_dir, model_name + \"_confusion_matrix.txt\"), header=None, index=None)\n",
    "    plt.show(block=False)\n",
    "    \n",
    "\n",
    "def graph_history(loss_history, acc_history, n_iterations, interval):\n",
    "    # create the figure\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax2 = ax1.twinx()\n",
    "    \n",
    "    # plot the stuff\n",
    "    iterations = interval * (np.arange(n_iterations // interval) + 1)\n",
    "    ax1.plot(iterations, loss_history, label=\"Loss\", color=\"c\")\n",
    "    ax2.plot(iterations, acc_history, label=\"Val. Accuracy\", color=\"m\")\n",
    "    \n",
    "    # also, indicate the iteration with the best loss\n",
    "    ax1.axvline(interval * (np.argmin(loss_history) + 1), color=\"k\", linestyle=\"--\")\n",
    "\n",
    "    # pretty-fy the plot\n",
    "    ax1.set_xlabel(\"Training Iteration\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax2.set_ylabel(\"Accuracy\")\n",
    "    fig.legend()\n",
    "\n",
    "    # plt.savefig(os.path.join(os.getcwd(), \"images\", \"history.png\"), dpi=100)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def save_model(model, model_name):\n",
    "    directory = os.path.join(model_dir)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    \n",
    "    # save architecture\n",
    "    with open(os.path.join(directory, model_name + \".json\"), \"w+\") as f:\n",
    "        f.write(model.to_json())\n",
    "    \n",
    "    # save weights\n",
    "    model.save_weights(os.path.join(directory, model_name + \"_weights.h5\"))\n",
    "\n",
    "def load_model(model_name):\n",
    "    directory = os.path.join(model_dir)\n",
    "\n",
    "    if os.path.exists(directory):\n",
    "        # load the architecture\n",
    "        json_file = open(os.path.join(directory, model_name + \".json\"))\n",
    "        model = model_from_json(json_file.read())\n",
    "        model.compile(loss=\"categorical_crossentropy\", optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "        json_file.close()\n",
    "\n",
    "        # load the weights and return the model\n",
    "        model.load_weights(os.path.join(directory, model_name + \"_weights.h5\"))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading snapshots... done!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vehicle</th>\n",
       "      <th>Capture</th>\n",
       "      <th>Data 1</th>\n",
       "      <th>Data 2</th>\n",
       "      <th>Data 3</th>\n",
       "      <th>Data 4</th>\n",
       "      <th>Data 5</th>\n",
       "      <th>Data 6</th>\n",
       "      <th>Data 7</th>\n",
       "      <th>Data 8</th>\n",
       "      <th>...</th>\n",
       "      <th>Data 1015</th>\n",
       "      <th>Data 1016</th>\n",
       "      <th>Data 1017</th>\n",
       "      <th>Data 1018</th>\n",
       "      <th>Data 1019</th>\n",
       "      <th>Data 1020</th>\n",
       "      <th>Data 1021</th>\n",
       "      <th>Data 1022</th>\n",
       "      <th>Data 1023</th>\n",
       "      <th>Data 1024</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>115565</th>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>176</td>\n",
       "      <td>216</td>\n",
       "      <td>50</td>\n",
       "      <td>...</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>109</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293329</th>\n",
       "      <td>109</td>\n",
       "      <td>109</td>\n",
       "      <td>76</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>109</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>242</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283066</th>\n",
       "      <td>105</td>\n",
       "      <td>105</td>\n",
       "      <td>39</td>\n",
       "      <td>178</td>\n",
       "      <td>38</td>\n",
       "      <td>212</td>\n",
       "      <td>39</td>\n",
       "      <td>188</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>184</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56658</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>127</td>\n",
       "      <td>226</td>\n",
       "      <td>128</td>\n",
       "      <td>8</td>\n",
       "      <td>30</td>\n",
       "      <td>128</td>\n",
       "      <td>...</td>\n",
       "      <td>38</td>\n",
       "      <td>30</td>\n",
       "      <td>58</td>\n",
       "      <td>30</td>\n",
       "      <td>229</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>96</td>\n",
       "      <td>228</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13759</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>127</td>\n",
       "      <td>226</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>176</td>\n",
       "      <td>123</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1026 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Vehicle  Capture  Data 1  Data 2  Data 3  Data 4  Data 5  Data 6  \\\n",
       "115565        3       14       0       0       0       0      13     176   \n",
       "293329      109      109      76      32       0      36     109       0   \n",
       "283066      105      105      39     178      38     212      39     188   \n",
       "56658         3        8      22       0     127     226     128       8   \n",
       "13759         3        4     255       0     255     255       0       0   \n",
       "\n",
       "        Data 7  Data 8  ...  Data 1015  Data 1016  Data 1017  Data 1018  \\\n",
       "115565     216      50  ...         80         10        109          0   \n",
       "293329       0      84  ...          0         84          0          1   \n",
       "283066       0       0  ...          0        184          0          0   \n",
       "56658       30     128  ...         38         30         58         30   \n",
       "13759      128       0  ...        150          0        127        226   \n",
       "\n",
       "        Data 1019  Data 1020  Data 1021  Data 1022  Data 1023  Data 1024  \n",
       "115565          0          9          0          0         14         96  \n",
       "293329          0          0          0         19        242         27  \n",
       "283066          0          0          0          0          0          0  \n",
       "56658         229         32          0         96        228        255  \n",
       "13759          80          0         34        176        123        168  \n",
       "\n",
       "[5 rows x 1026 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# root path\n",
    "filename = \"data/snapshots/unfiltered_snapshots.csv\"\n",
    "\n",
    "# load the csv\n",
    "print(\"Loading snapshots...\", end=\" \")\n",
    "col_names = pd.read_csv(filename, nrows=0).columns\n",
    "types_dict = {\"Vehicle\": int, \"Capture\": int}\n",
    "types_dict.update({col: int for col in col_names if col not in types_dict})\n",
    "df = pd.read_csv(filename, dtype=types_dict)  \n",
    "print(\"done!\")\n",
    "\n",
    "display(df.sample(frac=0.0001).head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter down to Stone's vehicles\n",
    "df = df[df[\"Vehicle\"] > 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into known/unknown sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Known vehicles   : [101 102 103 104 106 107 108 110]\n",
      "Unknown vehicles : [105 109 111]\n"
     ]
    }
   ],
   "source": [
    "# randomly select known vehicles\n",
    "all_vehicles = df[\"Vehicle\"].unique()\n",
    "known_vehicles = np.random.choice(all_vehicles, size=int(all_vehicles.shape[0] * 0.75), replace=False)\n",
    "\n",
    "# balance the dataset\n",
    "if BALANCED:\n",
    "    min_size = np.min(df[\"Vehicle\"].value_counts())\n",
    "    balanced_df = df.groupby(\"Vehicle\", group_keys=False).apply(pd.DataFrame.sample, n=min_size)\n",
    "\n",
    "# pick the desired dataframe\n",
    "samples = df if not BALANCED else balanced_df\n",
    "\n",
    "# split the dataframe into known/unknown\n",
    "known_df = samples[samples[\"Vehicle\"].isin(known_vehicles)]\n",
    "unknown_df = samples[~samples[\"Vehicle\"].isin(known_vehicles)]\n",
    "\n",
    "# save these\n",
    "known_vehicles = known_df[\"Vehicle\"].unique()\n",
    "unknown_vehicles = unknown_df[\"Vehicle\"].unique()\n",
    "\n",
    "print(\"Known vehicles   :\", known_vehicles)\n",
    "print(\"Unknown vehicles :\", unknown_vehicles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into (x, y) sets\n",
    "known_x = known_df.drop(columns=[\"Vehicle\", \"Capture\"])\n",
    "known_y = known_df[\"Vehicle\"]\n",
    "\n",
    "unknown_x = unknown_df.drop(columns=[\"Vehicle\", \"Capture\"])\n",
    "unknown_y = unknown_df[\"Vehicle\"]\n",
    "\n",
    "# scale the inputs\n",
    "known_x /= 255\n",
    "unknown_x /= 255\n",
    "\n",
    "# add the extra dimensions\n",
    "known_x = np.expand_dims(known_x, axis=2)\n",
    "unknown_x = np.expand_dims(unknown_x, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set\n",
      "(4115, 1024, 1)\n",
      "(1856, 1024, 1)\n",
      "(1812, 1024, 1)\n",
      "(2221, 1024, 1)\n",
      "(1709, 1024, 1)\n",
      "(2220, 1024, 1)\n",
      "(3041, 1024, 1)\n",
      "(2980, 1024, 1)\n",
      "\n",
      "Testing set\n",
      "(3806, 1024, 1)\n",
      "(2564, 1024, 1)\n",
      "(2101, 1024, 1)\n"
     ]
    }
   ],
   "source": [
    "# restructure the known vehicles\n",
    "print(\"Training set\")\n",
    "train_x = []\n",
    "\n",
    "for vehicle in known_vehicles:\n",
    "    samples = []\n",
    "    indices = np.where(known_y == vehicle)[0]\n",
    "    \n",
    "    for idx in indices:\n",
    "        samples.append(known_x[idx])\n",
    "    \n",
    "    train_x.append(np.stack(samples))\n",
    "    print(train_x[-1].shape)\n",
    "\n",
    "# restructure the unknown vehicles\n",
    "print(\"\\nTesting set\")\n",
    "test_x = []\n",
    "\n",
    "for vehicle in unknown_vehicles:\n",
    "    samples = []\n",
    "    indices = np.where(unknown_y == vehicle)[0]\n",
    "    \n",
    "    for idx in indices:\n",
    "        samples.append(unknown_x[idx])\n",
    "    \n",
    "    test_x.append(np.stack(samples))\n",
    "    print(test_x[-1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a one-shot batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# build a set of [n] sample pairs\n",
    "# for half of the pairs, the samples come from the same class\n",
    "# for the other half, the samples come from different classes\n",
    "def make_batch(n, samples):\n",
    "    # save these numbers\n",
    "    n_vehicles = len(samples)\n",
    "    length = 1024\n",
    "    width = 1\n",
    "    \n",
    "    # initialize two empty arrays for the input batch\n",
    "    x = [np.zeros((n, length, width)) for i in range(2)]\n",
    "    \n",
    "    # randomly select [n] vehicles\n",
    "    vehicles = np.random.choice(n_vehicles, size=(n,), replace=False)\n",
    "    \n",
    "    # for each sample pair...\n",
    "    for i in range(n):\n",
    "        # select the left vehicle\n",
    "        left_vehicle = vehicles[i]\n",
    "        \n",
    "        # select the sample of the left vehicle\n",
    "        left_sample = np.random.randint(0, len(samples[left_vehicle]))\n",
    "        \n",
    "        # copy the left sample into the batch\n",
    "        x[0][i, :, :] = samples[left_vehicle][left_sample]\n",
    "        \n",
    "        # select the right vehicle\n",
    "        # the first half of the pairs should have the same vehicle\n",
    "        if i >= n // 2:\n",
    "            right_vehicle = left_vehicle\n",
    "        \n",
    "        # the second half of the pairs should have different vehicles\n",
    "        else: \n",
    "            right_vehicle = (left_vehicle + np.random.randint(1, n_vehicles)) % n_vehicles\n",
    "        \n",
    "        # select the sample of the right vehicle\n",
    "        right_sample = np.random.randint(0, len(samples[right_vehicle]))\n",
    "        \n",
    "        # copy the right sample into the batch\n",
    "        x[1][i, :, :] = samples[right_vehicle][right_sample]\n",
    "    \n",
    "    # half of the targets should be [1] to indicate \"same class\"\n",
    "    # the other half should be [0] to indicate \"different class\"\n",
    "    y = np.zeros((n,))    \n",
    "    y[n // 2:] = 1\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test one-shot learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a set of [n] sample pairs consisting of random images of random characters\n",
    "# one pair should consist of two images from the same class\n",
    "# the remaining pairs should consist of images from different classes\n",
    "def make_oneshot_task(n, samples):\n",
    "    # save these numbers\n",
    "    n_vehicles = len(samples)\n",
    "    length = 1024\n",
    "    width = 1\n",
    "\n",
    "    # randomly select [n] vehicles\n",
    "    vehicles = np.random.choice(range(n_vehicles), size=(n,), replace=False)\n",
    "    \n",
    "    # make [n] copies of a random sample from a random test vehicle (this is the anchor)\n",
    "    test_vehicle = vehicles[0]\n",
    "    n_examples = len(samples[test_vehicle])\n",
    "    \n",
    "    # we'll need two different random numbers\n",
    "    test_sample, support_sample = np.random.choice(n_examples, replace=False, size=(2,))\n",
    "    test_samples = np.asarray([samples[test_vehicle][test_sample]] * n)\n",
    "    \n",
    "    # we'll need these\n",
    "    support_samples = []\n",
    "    \n",
    "    # select one random image from each of the [n] random classes\n",
    "    for i in vehicles:\n",
    "        n_examples = len(samples[i])\n",
    "        idx = np.random.randint(0, n_examples)\n",
    "        support_samples.append(samples[i][idx])\n",
    "    \n",
    "    # we want a numpy array\n",
    "    support_samples = np.stack(support_samples)\n",
    "    \n",
    "    # replace the test class's random image with the previously selected image\n",
    "    # we do this to guarantee that the test image for the test class\n",
    "    # is different from the support image for the test class\n",
    "    support_samples[0] = samples[test_vehicle][support_sample]\n",
    "    \n",
    "    # the target for the first pair is [1] because it contains two samples from the same vehicle\n",
    "    # the output for each of the remaining images is [0] because they come from different classes\n",
    "    targets = np.zeros((n,))\n",
    "    targets[0] = 1\n",
    "    \n",
    "    # shuffle the images\n",
    "    targets, test_samples, support_samples = shuffle(targets, test_samples, support_samples)\n",
    "    \n",
    "    # we want a set of image pairs, not two sets of images\n",
    "    pairs = [test_samples, support_samples]\n",
    "\n",
    "    # return the stuff\n",
    "    return pairs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate [k] instances of [n]-way one-shot learning\n",
    "def test_oneshot_model(n, k, model, samples):\n",
    "    print(\"Evaluating model on {} random {}-way one-shot learning tasks...\".format(k,n))\n",
    "    \n",
    "    # count the number of correct predictions\n",
    "    n_correct = 0\n",
    "    \n",
    "    # for each task...\n",
    "    for i in range(k):\n",
    "        # generate test set and predict outputs\n",
    "        inputs, targets = make_oneshot_task(n, samples)\n",
    "        preds = model.predict(inputs)\n",
    "        \n",
    "        # check prediction\n",
    "        if np.argmax(preds) == np.argmax(targets):\n",
    "            n_correct += 1\n",
    "    \n",
    "    # output stats\n",
    "    percent_correct = 100. * n_correct / k\n",
    "    print(\"Average {}-way one-shot learning accuracy: {:.2f}%\".format(n, percent_correct))\n",
    "    return percent_correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build siamese neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(size):\n",
    "    return np.random.normal(loc=0.0, scale=1e-2, size=size)\n",
    "\n",
    "def initialize_bias(size):\n",
    "    return np.random.normal(loc=0.5, scale=1e-2, size=size)\n",
    "\n",
    "def build_siamese_model(filters, input_shape):\n",
    "    # siamese networks have two inputs\n",
    "    left_input = Input(shape=input_shape)\n",
    "    right_input = Input(shape=input_shape)\n",
    "\n",
    "    # build the model\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters[0], 4, input_shape=input_shape, padding=\"same\",\n",
    "                     kernel_initializer=initialize_weights, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(MaxPooling1D())\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters[1], 4, input_shape=input_shape, padding=\"same\",\n",
    "                     kernel_initializer=initialize_weights, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(MaxPooling1D())\n",
    "    \n",
    "    for i in filters[2:]:\n",
    "        model.add(Conv1D(i, 4, padding=\"same\", kernel_initializer=initialize_weights,\n",
    "            bias_initializer=initialize_bias, kernel_regularizer=l2(2e-4)))\n",
    "        model.add(Conv1D(i, 4, padding=\"same\", kernel_initializer=initialize_weights,\n",
    "            bias_initializer=initialize_bias, kernel_regularizer=l2(2e-4)))\n",
    "        model.add(Conv1D(i, 4, padding=\"same\", kernel_initializer=initialize_weights,\n",
    "            bias_initializer=initialize_bias, kernel_regularizer=l2(2e-4)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(MaxPooling1D())\n",
    "        \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation=\"sigmoid\", kernel_initializer=initialize_weights,\n",
    "                    bias_initializer=initialize_bias, kernel_regularizer=l2(1e-3)))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # generate the encodings for the two samples\n",
    "    left_encoding = model(left_input)\n",
    "    right_encoding = model(right_input)\n",
    "    \n",
    "    # add a custom layer to compute the absolute difference between the encodings\n",
    "    l1_layer = Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))\n",
    "    l1_distance = l1_layer([left_encoding, right_encoding])\n",
    "    \n",
    "    # add a dense layer to compute the similarity score\n",
    "    prediction = Dense(1, activation=\"sigmoid\", bias_initializer=initialize_bias)(l1_distance)\n",
    "    \n",
    "    # connect the inputs to the outputs\n",
    "    siamese_net = Model(inputs=[left_input, right_input], outputs=prediction)\n",
    "    \n",
    "    # compile the model\n",
    "    siamese_net.compile(loss=\"binary_crossentropy\", optimizer=Adam(lr=6e-5))\n",
    "    \n",
    "    # return the model\n",
    "    return siamese_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test one-shot learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "interval = 200  # interval for evaluating on one-shot tasks\n",
    "batch_size = len(known_vehicles)  # number of image pairs in each training batch\n",
    "n_iterations = 25000  # number of training iterations\n",
    "n_vehicles = len(unknown_vehicles)  # number of classes for each one-shot test\n",
    "n_tasks = 250  # number of one-shot tasks to use for validation\n",
    "best_acc = -1  # best validation accuracy during training\n",
    "\n",
    "# different model configurations\n",
    "filter_sets = [\n",
    "    [32, 32, 64, 32, 128, 64, 256, 128, 512],\n",
    "]\n",
    "\n",
    "# for each configuration...\n",
    "for filters in filter_sets:\n",
    "    # build the model\n",
    "    print(filters)\n",
    "    model = build_siamese_model(filters=filters, input_shape=(1024,1))\n",
    "    model.summary()\n",
    "\n",
    "    # train the model\n",
    "    # how long does this take?\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(\"Training model\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # we'll need these\n",
    "    loss_history = []\n",
    "    acc_history = []\n",
    "\n",
    "    # for each training iteration...\n",
    "    for i in range(1, n_iterations + 1):\n",
    "        # make a training batch and train model using this batch\n",
    "        inputs, targets = make_batch(batch_size, train_x)\n",
    "        loss = model.train_on_batch(inputs, targets)\n",
    "\n",
    "        # if it's time to evaluate the model...\n",
    "        if i % interval == 0:\n",
    "            # output some stats\n",
    "            print(\"\\n{}\\n\".format(\"-\" * 20))\n",
    "            print(\"Time for {} iterations: {:.2f} mins\".format(i, (time.time() - start_time) / 60.))\n",
    "            print(\"Train loss: {:.6f}\".format(loss))\n",
    "\n",
    "            # save model\n",
    "            # model.save_weights(os.path.join(model_dir, \"weights.{}.h5\".format(i)))\n",
    "\n",
    "            # evaluate model\n",
    "            val_acc = test_oneshot_model(n_vehicles, n_tasks, model, test_x)\n",
    "            loss_history.append(loss)\n",
    "            acc_history.append(val_acc)\n",
    "\n",
    "            # update best accuracy\n",
    "            if val_acc >= best_acc:\n",
    "                print(\"Current best: {:.2f}; previous best: {:.2f}\".format(val_acc, best_acc))\n",
    "                best_acc = val_acc\n",
    "\n",
    "    print(\"Elapsed time: {:.2f} minutes\".format((time.time() - start_time) / 60.))\n",
    "\n",
    "    # show the training loss and accuracy over time\n",
    "    graph_history(loss_history, acc_history, n_iterations, interval)\n",
    "\n",
    "    # stats for best iteration-\n",
    "    best_iteration = interval * (np.argmin(loss_history) + 1)\n",
    "    print(\"Best iteration :\", best_iteration)\n",
    "    print(\"Training loss  :\", np.min(loss_history))\n",
    "    print(\"Val. accuracy  :\", acc_history[np.argmin(loss_history)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # hyperparameters\n",
    "# interval = 200  # interval for evaluating on one-shot tasks\n",
    "# batch_size = len(known_vehicles)  # number of image pairs in each training batch\n",
    "# n_iterations = 25000  # number of training iterations\n",
    "# n_vehicles = len(unknown_vehicles)  # number of classes for each one-shot test\n",
    "# n_tasks = 250  # number of one-shot tasks to use for validation\n",
    "# best_acc = -1  # best validation accuracy during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # how long does this take?\n",
    "# start_time = time.time()\n",
    "\n",
    "# print(\"Training model\")\n",
    "# print(\"-\" * 50)\n",
    "\n",
    "# loss_history = []\n",
    "# acc_history = []\n",
    "\n",
    "# # for each training iteration...\n",
    "# for i in range(1, n_iterations + 1):\n",
    "#     # make a training batch and train model using this batch\n",
    "#     inputs, targets = make_batch(batch_size, train_x)\n",
    "#     loss = model.train_on_batch(inputs, targets)\n",
    "\n",
    "#     # if it's time to evaluate the model...\n",
    "#     if i % interval == 0:\n",
    "#         # output some stats\n",
    "#         print(\"\\n{}\\n\".format(\"-\" * 20))\n",
    "#         print(\"Time for {} iterations: {:.2f} mins\".format(i, (time.time() - start_time) / 60.))\n",
    "#         print(\"Train loss: {:.6f}\".format(loss))\n",
    "\n",
    "#         # save model\n",
    "#         # model.save_weights(os.path.join(model_dir, \"weights.{}.h5\".format(i)))\n",
    "\n",
    "#         # evaluate model\n",
    "#         val_acc = test_oneshot_model(n_vehicles, n_tasks, model, test_x)\n",
    "#         loss_history.append(loss)\n",
    "#         acc_history.append(val_acc)\n",
    "\n",
    "#         # update best accuracy\n",
    "#         if val_acc >= best_acc:\n",
    "#             print(\"Current best: {:.2f}; previous best: {:.2f}\".format(val_acc, best_acc))\n",
    "#             best_acc = val_acc\n",
    "\n",
    "# print(\"Elapsed time: {:.2f} minutes\".format((time.time() - start_time) / 60.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # show the model's information\n",
    "# print(filters)\n",
    "# model.summary()\n",
    "\n",
    "# # show the training loss and accuracy over time\n",
    "# graph_history(loss_history, acc_history, n_iterations, interval)\n",
    "\n",
    "# # stats for best iteration-\n",
    "# best_iteration = interval * (np.argmin(loss_history) + 1)\n",
    "# print(\"Best iteration :\", best_iteration)\n",
    "# print(\"Training loss  :\", np.min(loss_history))\n",
    "# print(\"Val. accuracy  :\", acc_history[np.argmin(loss_history)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
