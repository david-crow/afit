{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Part 2\n",
    "Goal: Implement Batch Normalization for weights (per layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functions\n",
    "DEBUG = True\n",
    "def debug(*kargs):\n",
    "    if DEBUG:\n",
    "        print(*kargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1. / (1. + np.exp(-x))\n",
    "\n",
    "\n",
    "def dSigmoid(x):  # derivative of sigmoid\n",
    "    s = sigmoid(x)\n",
    "    return np.multiply(s, (1-s))\n",
    "\n",
    "\n",
    "def relu(z):  # rectified linear unit activation\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "\n",
    "def dRelu(z):\n",
    "    \"\"\" \n",
    "    Derivative of Rectified Linear Unit\n",
    "\n",
    "    \"\"\"\n",
    "    return 1 * (z > 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Coding steps:\n",
    "\n",
    " Implement Batch Normalization for backprop in Layer subclass function <def normalize_activation(self, activation)>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN:\n",
    "    data = []\n",
    "    layers = []\n",
    "    inputWidth = 1\n",
    "    outputWidth = 1\n",
    "\n",
    "    class Layer:\n",
    "\n",
    "        \"\"\"class defining the elements of an ANN layer\"\"\"\n",
    "\n",
    "        @staticmethod\n",
    "        def gradl2norm(weight_vals):\n",
    "            \"\"\"returns the gradient of the l2 norm with respect to the weights\"\"\"\n",
    "            return weight_vals\n",
    "    \n",
    "        @staticmethod\n",
    "        def gradl1norm(weight_vals):\n",
    "            \"\"\"returns the gradient of the l1 norm with respect to the weights\"\"\"\n",
    "            return np.sign(weight_vals)\n",
    "\n",
    "        @staticmethod\n",
    "        def l2norm(vals):\n",
    "            \"\"\"returns the l2 norm of the vals\"\"\"\n",
    "            return np.linalg.norm(vals,ord=2)\n",
    "    \n",
    "        @staticmethod\n",
    "        def l1norm(vals):\n",
    "            \"\"\"returns the l1 norm of the vals\"\"\"\n",
    "            return np.linalg.norm(vals,ord=1)\n",
    "        \n",
    "        def __init__(self):\n",
    "            self.w = []\n",
    "            self.b = []\n",
    "            self.lam = 0  #for weight regularization\n",
    "            self.weightRegFunction = self.l2norm  #placeholder for regularization function\n",
    "            self.weightRegGradFunction = self.gradl2norm   #placeholder for regularization function\n",
    "            self.batchNormalization = False  #batchNormalization for backpropagation\n",
    "            self.vel_w=[] # for standard momentum of weight gradients\n",
    "            self.vel_b=[] # for standard momentum of bias gradients\n",
    "            self.nodecount = []\n",
    "            self.activation_fcn = []\n",
    "            self.activation_fcn_derivative = []\n",
    "            self.orderNumber = []\n",
    "            self.previous = None  # link to previous layer\n",
    "            self.next = None  # link to next layer\n",
    "            \n",
    "\n",
    "        def set_weights(self, w, b):\n",
    "            \"\"\"set the weights and bias for the layer.  Layer weights should have dimesion: (thislayer_nodecount, previouslayer_nodecount)\n",
    "            the dimension of the bias should be (thislayer_nodecount,1)\"\"\"\n",
    "            self.w = w\n",
    "            self.b = b\n",
    "            return self\n",
    "        \n",
    "        def initialize_weights(self, glorot=False, seed=None ):\n",
    "            assert self.w.size > 0\n",
    "            assert self.b.size > 0\n",
    "            np.random.seed(seed)\n",
    "            if glorot:  #use glorot initialization\n",
    "                n = self.w.shape[0]\n",
    "                m = self.w.shape[1]\n",
    "                self.w = np.zeros(self.w.shape)   #placeholder\n",
    "                # compute the edges of the glorot-specified uniform distribution and then set self.w to a uniform dist\n",
    "                # note: use np.random.uniform(loweredge,upperedge,size)\n",
    "                u_edge = np.sqrt(6/(m+n))\n",
    "                self.w = np.random.uniform(low=-u_edge,high=u_edge,size=(self.w.shape))\n",
    "            else:\n",
    "                self.w = np.random.normal(size=(self.w.shape))  # hidden weight matrix [rows = to, columns = from]\n",
    "            self.b = np.zeros(self.b.shape)  # hidden biases (column vector)\n",
    "            \n",
    "        def initialize_velocity(self):\n",
    "            assert self.w.size > 0\n",
    "            assert self.b.size > 0\n",
    "            self.vel_w = np.zeros(self.w.shape)  # same shape as hidden weight matrix [rows = to, columns = from]\n",
    "            self.vel_b = np.zeros(self.b.shape)  # same shape as hidden biases (column vector)\n",
    "        \n",
    "\n",
    "        def set_lambda(self, lam):\n",
    "            self.lam = lam\n",
    "\n",
    "        def set_weightRegFunction(self, fcn, d_fcn):\n",
    "            self.weightRegFunction = fcn\n",
    "            self.weightRegGradFunction = d_fcn\n",
    "            \n",
    "        def set_activation(self, activation_fcn):\n",
    "            self.activation_fcn = activation_fcn\n",
    "            return self\n",
    "\n",
    "        def set_activation_deriv(self, activation_fcn):\n",
    "            if activation_fcn == sigmoid:\n",
    "                self.activation_fcn_derivative = copy.deepcopy(dSigmoid)\n",
    "            elif activation_fcn == relu:\n",
    "                self.activation_fcn_derivative = copy.deepcopy(dRelu)\n",
    "            else:\n",
    "                self.activation_fcn_derivative = None\n",
    "                \n",
    "        def display_params(self):\n",
    "            \"\"\"displays the weights and biases of the layer (rows = to, colums = from)\"\"\"\n",
    "            for outNum in range(self.w.shape[0]):\n",
    "                print(self.w[outNum,:], \"  \", self.b[outNum])\n",
    "        \n",
    "\n",
    "        def compute_pre_activation(self, inputs):\n",
    "            net = np.dot(self.w, inputs) + self.b\n",
    "            return net\n",
    "\n",
    "        def compute_bias_gradient(self, gradient):\n",
    "            g = np.mean(gradient, axis=1)[:, np.newaxis]  # no regularization\n",
    "            return g\n",
    "\n",
    "        def compute_weight_gradient(self, inputs, gradient):\n",
    "            g = np.dot(gradient, inputs.T)\n",
    "            g = g/inputs.shape[1]   #divide by m (batchsize)\n",
    "            return g\n",
    "\n",
    "        def compute_activation(self, net):\n",
    "            return self.activation_fcn(net)\n",
    "\n",
    "        def compute_activation_derivative(self, net):\n",
    "            return self.activation_fcn_derivative(net)\n",
    "\n",
    "        def compute_activation_gradient(self, net, gradient):\n",
    "            g = np.multiply(gradient, net)\n",
    "            return g\n",
    "\n",
    "        def compute_forward(self, inputs):\n",
    "            \"\"\"Returns layer ouput from input (shape = [nodeCount, input]) of the weighted input plus bias\n",
    "            input shape must be [lastlayer_nodeCount, samples] or [featurecount, samplecount] \"\"\"\n",
    "            net = self.compute_pre_activation(self, inputs)\n",
    "            layer_out = self.compute_activation(net)\n",
    "            return layer_out\n",
    "\n",
    "        def regularization_grad_weights(self,vals):\n",
    "            \"\"\"computes the regularization cost for the current layer weights\"\"\"\n",
    "            mylam = self.lam\n",
    "            myregs = self.weightRegGradFunction(vals)\n",
    "            return mylam*myregs\n",
    "        \n",
    "        def normalize_activation(self, activation):\n",
    "            \"\"\"computes and returns normalized activations for use in batch normalization computations\"\"\"\n",
    "            \n",
    "            batchsize = activation.shape[1]\n",
    "            activation_mu = np.zeros(shape=(activation.shape[0],1)) #placeholder must be shape ([nodecount, 1])\n",
    "            delta = 10e-8\n",
    "            activation_sigma = np.ones(shape=(activation.shape[0],1)) #placeholder must be shape ([nodecount, 1])\n",
    "            activation_norm = activation #placholder must be same shape as activation ([nodecount, batchsize])\n",
    "           \n",
    "            ############### STUDENT CODE FOR BATCH NORMALIZATION ###############\n",
    "           \n",
    "            #compute the mean of the activation: activation_mu (shape=[nodecount, 1])\n",
    "            \n",
    "            activation_mu = np.mean(activation, axis=1, keepdims=True)\n",
    "            \n",
    "            #subtract the activation_mu from the activation matrix; must be shame shape as activation\n",
    "            #hint, use np.subtract() to broadcast\n",
    "            \n",
    "            numerator = np.subtract(activation, activation_mu)\n",
    " \n",
    "            #compute the sigma of the activation (shape=[nodecount, 1]) \n",
    "\n",
    "            activation_sigma = np.sqrt(delta + np.diag(np.dot(numerator, numerator.T)) / batchsize)\n",
    "            activation_sigma = activation_sigma.reshape(-1,1)\n",
    "    \n",
    "            #compute new normalized activations h_prime from original activation ([nodecount, batchsize])\n",
    "        \n",
    "            activation_norm = np.divide(numerator, activation_sigma)\n",
    "\n",
    "            ########################## END STUDENT CODE ########################\n",
    "            \n",
    "            return activation_norm  #must be of shape [nodecount,batchsize]\n",
    "\n",
    "        def compute_layer_gradients(self, net, activation, gradient):\n",
    "            \"\"\" computes the loss gradient with respect to desired output of the layer\n",
    "            a set of desired targets is assumed to be matrix of shape [nodecount, samples]: SGD will have [nodecount,1]\n",
    "            hidden_inputs is assumed to be a matrix of shape [hiddenNodeCount, samples]\n",
    "            \n",
    "            This follows algorithm 6.4 line by line in the book!\n",
    "            \n",
    "            \"\"\"\n",
    "            \n",
    "            # f'(a(k))\n",
    "            d_activation = self.compute_activation_derivative(net)  # derivative of sigmoid:  shape = [NodeCount, samples]\n",
    "            \n",
    "            # g <- g * f'(a(k))\n",
    "            g_loss = self.compute_activation_gradient(d_activation, gradient)  # shape = [NodeCount, samples]  for outer layer\n",
    "            \n",
    "            # Delta_b(k) J = g (Take the mean across all 4 samples (batch))\n",
    "            g_loss_b = self.compute_bias_gradient(g_loss)  # mean gradient with respect to BIAS, shape = [NodeCount, 1]\n",
    "            \n",
    "            # Delta w(k) J = g * h(k-1) +lam*regularizationGrad w.r.t weights\n",
    "            if self.batchNormalization == False:\n",
    "                g_loss_w = self.compute_weight_gradient(activation, g_loss) + self.regularization_grad_weights(self.w) # [thisLayerNodecount,prevLayerOutputcount]  \n",
    "                #NOTE - regularization grad weights NOT WORKING YET\n",
    "            else: #use batch normalization\n",
    "                n_activation = self.normalize_activation(activation)\n",
    "                #debug(\"--normalized activation shape:\", n_activation.shape)\n",
    "                g_loss_w = self.compute_weight_gradient(n_activation, g_loss) + self.regularization_grad_weights(self.w) # [thisLayerNodecount,prevLayerOutputcount]  \n",
    "                \n",
    "            \n",
    "            # g <- W(k).T * g\n",
    "            g_loss_backprop = np.dot(self.w.T, g_loss)  # gradient to propagate back, shape = [hiddenNodeCount,samples]\n",
    "            \n",
    "            return g_loss_w, g_loss_b, g_loss_backprop\n",
    "\n",
    "        def update_Layer(self, weightUpdate, biasUpdate, momentum=0):\n",
    "            \"\"\"Update weights and biases. weightUpdate is shape [thisLayerNodecount,prevLayerOutputcount]; biasUpdate is shape [thisLayerNodecount,1]\"\"\"\n",
    "            if momentum == 0:  #note this if-else statement not required if written as math eq with momentum & velocity >> more efficient!\n",
    "                self.w = self.w + weightUpdate\n",
    "                self.b = self.b + biasUpdate\n",
    "            else:  #note this if-else statement not required if written as math eq with momentum & velocity\n",
    "                # need to compute the new values for self.w and self.b using momentum\n",
    "                # note: momentum is passed in but velocity must be updated & stored in self.vel_w and self.vel_b\n",
    "                self.vel_w = momentum*self.vel_w+weightUpdate\n",
    "                self.vel_b = momentum*self.vel_b+biasUpdate\n",
    "                self.w = self.w + self.vel_w\n",
    "                self.b = self.b + self.vel_b\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        self.data = []\n",
    "        self.layers = []\n",
    "        self.inputWidth = 1\n",
    "        self.outputWidth = 1\n",
    "    \n",
    "        \n",
    "    def set_input_width(self, inputWidth):\n",
    "        \"\"\"defines the input layer width for the network\"\"\"\n",
    "        self.inputWidth = inputWidth\n",
    "\n",
    "    def add_layer(self, nodecount=1, activation_fcn=relu):\n",
    "        \"\"\"adds a layer to the neural network and returns the layer\"\"\"\n",
    "        oldLayerCount = len(self.layers)\n",
    "        thislayer = ANN.Layer()\n",
    "        thislayer.orderNumber = oldLayerCount + 1\n",
    "        if oldLayerCount > 0:  # other layers have been added already\n",
    "            lastLayer = self.layers[-1]\n",
    "            lastLayer.display_params()\n",
    "            thislayer.previous = lastLayer\n",
    "            lastLayer.next = thislayer\n",
    "            layerInputSize = lastLayer.w.shape[0]\n",
    "        else:  # this will be the first layer\n",
    "            layerInputSize = self.inputWidth\n",
    "            \n",
    "        thislayer.w = np.zeros((nodecount, layerInputSize))  #[NODECOUNT,FROM]\n",
    "        thislayer.b = np.zeros((nodecount, 1 )) #[NODECOUNT,FROM]\n",
    "        thislayer.vel_w = np.zeros(thislayer.w.shape)  # same shape as hidden weight matrix [rows = to, columns = from]\n",
    "        thislayer.vel_b = np.zeros(thislayer.b.shape)  # same shape as hidden biases (column vector)\n",
    "\n",
    "        thislayer.activation_fcn = activation_fcn\n",
    "        thislayer.set_activation_deriv(activation_fcn)\n",
    "        self.outputWidth = nodecount\n",
    "        self.layers = self.layers + [thislayer]\n",
    "        return thislayer\n",
    "    \n",
    "    def initialize(self, glorot = False, batchNormalization=False, seed = None):\n",
    "        \"\"\"initialize weights & biases & velocity: overwrites current network parameters\"\"\"\n",
    "        for layer in self.layers:\n",
    "            layer.initialize_weights(glorot=glorot, seed=seed)\n",
    "            layer.initialize_velocity()\n",
    "            layer.batchNormalization = batchNormalization  #note that all layers will have this True or False when initialized\n",
    "            \n",
    "    def setL1weightNormalization(self,lam=0):\n",
    "        for idx,layer in enumerate(self.layers):\n",
    "            layer.set_lambda(lam)\n",
    "            layer.set_weightRegFunction(layer.l1norm,layer.gradl1norm)\n",
    "            print(\" Set Layer \", idx,\" weightNorm to gradl1norm with lambda = \", lam)\n",
    "            \n",
    "\n",
    "    def setL2weightNormalization(self,lam=0):\n",
    "        for idx,layer in enumerate(self.layers):\n",
    "            layer.set_lambda(lam)\n",
    "            layer.set_weightRegFunction(layer.l2norm,layer.gradl2norm )\n",
    "            print(\" Set Layer \", idx,\" weightNorm to gradl2norm with lambda = \", lam)\n",
    "\n",
    "            \n",
    "    def summary(self):\n",
    "        \"\"\"displays a summary of the model\"\"\"\n",
    "        tot_train_parameters = 0\n",
    "        print(\"\\n\")\n",
    "        print(\"Layer     Inshape     Outshape     Param #     LambdaReg\")\n",
    "        print(\"==========================================================\")\n",
    "        for lnum, layer in enumerate(self.layers):\n",
    "            inshape = layer.w.shape[1]\n",
    "            weightCount = layer.w.shape[0]*layer.w.shape[1]  #assume fully connected\n",
    "            biasCount = layer.b.shape[0]\n",
    "            thislayerparams = weightCount+biasCount\n",
    "            tot_train_parameters += thislayerparams\n",
    "            lam = layer.lam\n",
    "            print(\"% 3d       % 3d         % 3d         %3d         %3f\" %(lnum,inshape,biasCount,thislayerparams,lam))\n",
    "        print(\"==========================================================\")\n",
    "        print(\"total trainable params: \",tot_train_parameters )\n",
    "        \n",
    "    def display_params(self):\n",
    "        \"\"\"displays the weights and biases of the network (rows = to, colums = from)\"\"\"\n",
    "        print(\"\\n\")\n",
    "        print(\"input width: \", self.inputWidth)\n",
    "        for lnum, layer in enumerate(self.layers):\n",
    "            print(\"Layer \",lnum)\n",
    "            layer.display_params()\n",
    "        print(\"output width: \", layer.w.shape[0])\n",
    "                \n",
    "                \n",
    "    def forwardPropagation(self, inputs):\n",
    "        \"\"\"Compute forward pass of two layer network\n",
    "        inputs are assumed to be (shape=[sampleCount,featureCount])\n",
    "        returns a matrix of raw outputs with one row of output per node (shape=[sampleCount, outputNodeCount])\n",
    "        Internal matrices are shaped for efficiency to avoid internal transposes (columns hold observations/samples) \"\"\"\n",
    "\n",
    "        # inputs and outputs will be transposed for efficiency during forwardPropagation and untransposed before returning\n",
    "\n",
    "        nets = []\n",
    "        activations = []\n",
    "        layer_input = inputs.T\n",
    "\n",
    "        for lnum, layer in enumerate(self.layers):\n",
    "            # inputs = inputs + inputs\n",
    "            layer_net = layer.compute_pre_activation(layer_input)\n",
    "            nets.append(layer_net)\n",
    "\n",
    "            layer_out = layer.compute_activation(layer_net)\n",
    "            activations.append(layer_out)\n",
    "\n",
    "            layer_input = layer_out\n",
    "        raw_output = layer_out.T\n",
    "        return raw_output, inputs, nets, activations\n",
    "\n",
    "    def backPropagation(self, inputs, desiredOutputs, learningRate, momentum=0):\n",
    "        w_grads = []\n",
    "        b_grads = []\n",
    "        # store nets and activations for each layer\n",
    "        raw_output, _, nets, activations = self.forwardPropagation(inputs)\n",
    "        layer_desired_out = desiredOutputs\n",
    "\n",
    "        # Note: This is only part of the gradient\n",
    "        layer_grad = desiredOutputs - raw_output\n",
    "        layer_grad = layer_grad.T  #in order to match expectation for last layer output\n",
    "        prev_layer_outputs = [inputs.T] + activations  #insert inputs onto activation stream for easy computations\n",
    "\n",
    "        #  computation of full gradient handled inside the loop below\n",
    "        for lnum, layer in reversed(list(enumerate(self.layers))):\n",
    "            #get the input to this layer\n",
    "            curr_layer_input=prev_layer_outputs[lnum]\n",
    "            #get the gradients for the layer    \n",
    "            w_grad, b_grad, loss_grad = layer.compute_layer_gradients(nets[lnum], curr_layer_input, layer_grad)    \n",
    "\n",
    "            layer.update_Layer(w_grad * learningRate, b_grad * learningRate, momentum=momentum)\n",
    "            layer_grad = loss_grad\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        \"\"\"Compute predictions using forward propagation for single binary classification at threshold\n",
    "        X is a standard dataFrame without biases (shape=[observationCount,featureCount])\n",
    "        returns a standard column vector of binary predictions in {0,1}: (shape=[observationCount, 1])\"\"\"\n",
    "        raw_predictions, net_inputs, net_lst, activation_lst = self.forwardPropagation(X)\n",
    "        preds = raw_predictions > threshold\n",
    "        return preds\n",
    "\n",
    "    def compute_loss(self, inputs, desired_targets):\n",
    "        \"\"\"computes the (scalar) loss using MSE of a set of targets and sigmoid outputs\n",
    "        inputs is assumed to be a matrix of shape [samples, features]\n",
    "         desired_targets is assumed to be a matrix of shape [samples, 1]\"\"\"\n",
    "        raw_outputs = self.forwardPropagation(inputs)[0]\n",
    "        error = desired_targets - raw_outputs\n",
    "        mse = np.dot(error.T, error) / error.size\n",
    "        return mse\n",
    "\n",
    "    def fit(self, tngInputs, tngTargets, valInputs, valTargets, learningRate, learningRateDecay,\n",
    "            batchsize = 1, momentum=0, valPatience=0, tolerance=1e-2, maxEpochs = 100):\n",
    "        \"\"\"fit model to map tngInputs to tngTargets. If valPatience > 0 then use early stopping on valInputs & valTargets\n",
    "        returns training loss history and val loss history \"\"\"\n",
    "        done = False\n",
    "        tng_loss_history = []\n",
    "        val_loss_history = []\n",
    "        print(\"Training Model...\")\n",
    "        epoch = 0\n",
    "        #get current val performance\n",
    "        valPreds = self.predict(valInputs)\n",
    "        prev_val_loss = np.asscalar(self.compute_loss(valInputs, valTargets))\n",
    "        val_loss_history.append(prev_val_loss)\n",
    "        val_epochs_nonimproved = 0\n",
    "        training_count = tngInputs.shape[0]\n",
    "        if batchsize>training_count: batchsize=training_count #prevent sampling beyond training size\n",
    "            \n",
    "        \n",
    "        while not done:\n",
    "            epoch+=1\n",
    "            if epoch>maxEpochs: \n",
    "                done = True\n",
    "            learningRate = learningRate * learningRateDecay\n",
    "            tngPreds = self.predict(tngInputs)\n",
    "            tngCorrect = tngTargets == tngPreds\n",
    "            curr_train_loss = np.asscalar(self.compute_loss(tngInputs, tngTargets))\n",
    "            tng_loss_history.append(curr_train_loss)\n",
    "            #evaluate validation performance\n",
    "            valPreds = self.predict(valInputs)\n",
    "            cur_val_loss = np.asscalar(self.compute_loss(valInputs, valTargets))\n",
    "\n",
    "            if cur_val_loss < tolerance:  #regular stopping\n",
    "                done = True\n",
    "                print(\" --- Regular Stopping due to val loss < tolerance; val loss:\", cur_val_loss)\n",
    "                break\n",
    "            \n",
    "            # run an epoch of backprop\n",
    "            #shuffle the indexes of the inputs & targets simultaneously\n",
    "            order=np.random.permutation(training_count)\n",
    "            #debug(\"range check \", np.arange(training_count))\n",
    "            #debug(\"tng count:\", training_count, \";   tng index order: \", order)\n",
    "            first_tng_index=0\n",
    "            last_tng_index = batchsize\n",
    "            \n",
    "            tinp = tngInputs[order]\n",
    "            ttar = tngTargets[order]\n",
    "            \n",
    "            while last_tng_index<=training_count:\n",
    "                #get a batch\n",
    "                batchIn = tinp[first_tng_index:last_tng_index,:]\n",
    "                batchTar = ttar[first_tng_index:last_tng_index,:]\n",
    "                #train on the batch using backprop\n",
    "                self.backPropagation(batchIn, batchTar, learningRate=learningRate, momentum=momentum)\n",
    "                first_tng_index+=batchsize\n",
    "                last_tng_index+=batchsize\n",
    "                #handle mis-aligned training set sizes\n",
    "                if first_tng_index < training_count-1 and last_tng_index>training_count-1:\n",
    "                    batchIn=tinp[first_tng_index:training_count,:]\n",
    "                    batchTar=ttar[first_tng_index:training_count,:]\n",
    "                    self.backPropagation(batchIn, batchTar, learningRate=learningRate, momentum=momentum)\n",
    "            \n",
    "\n",
    "            # Early Stopping via VAL loss improvement\n",
    "            # if validation loss has not improved in patience epochs then stop\n",
    "            if cur_val_loss < prev_val_loss:\n",
    "                val_epochs_nonimproved = 0\n",
    "                prev_val_loss = cur_val_loss\n",
    "            else:\n",
    "                val_epochs_nonimproved+=1\n",
    "                if valPatience > 0 and val_epochs_nonimproved > valPatience :\n",
    "                    print(\" --- EARLY STOPPING ACTIVATED AT val_epochs_nonimproved =  \",val_epochs_nonimproved)\n",
    "                    done=True\n",
    "\n",
    "            val_loss_history.append(cur_val_loss)\n",
    "            \n",
    "        print(\"Training Complete!\")\n",
    "\n",
    "        return tng_loss_history, val_loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showDatapoints2d(featureData,labelData,title=\"Datapoints\"):\n",
    "    # add annotated points to show where the boolean inputs lie on the graph\n",
    "    #fig, ax = plt.subplots()\n",
    "    plt.figure(figsize=(10,10))\n",
    "    xmin, xmax = np.min(featureData[:, 0]) - 0.1, np.max(featureData[:, 0]) + 0.1\n",
    "    ymin, ymax = np.min(featureData[:, 1]) - 0.1, np.max(featureData[:, 1]) + 0.1\n",
    "\n",
    "    plt.scatter(featureData[:, 0], featureData[:, 1], color='b', alpha=0.1)\n",
    "    for i, txt in enumerate(labelData):\n",
    "        plt.annotate(np.asscalar(txt), (featureData[i, 0], featureData[i, 1]))\n",
    "\n",
    "        # adjust the axis & add labels for the graph\n",
    "    plt.axis([xmin, xmax, ymin, ymax])\n",
    "    plt.ylabel('x2')\n",
    "    plt.xlabel('x1')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def makeDecisionBoundaryBool2(model, featureData, labelData, title):\n",
    "    '''Build decision boundary figrue for 2-input, 1-output boolean logic functions\n",
    "    Note that this assumes a hard sigmoid was used and establishes a cutoff at 0.5\n",
    "    for predicting 0 or 1'''\n",
    "    cutoff = 0.5  # 0.5 for Sigmoid. 0.0 for TANH\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    xmin, xmax = np.min(featureData[:, 0]) - 0.1, np.max(featureData[:, 0]) + 0.1\n",
    "    ymin, ymax = np.min(featureData[:, 1]) - 0.1, np.max(featureData[:, 1]) + 0.1\n",
    "\n",
    "    # Create filled countour map to color both sides of the boundary\n",
    "    x = np.linspace(xmin, xmax, 200)\n",
    "    y = np.linspace(ymin, ymax, 200)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    grid = np.c_[X.ravel(), Y.ravel()]\n",
    "    preds = model.predict(grid)  # get predictions\n",
    "    z = preds.reshape(X.shape) > cutoff  # cutoff on predictions to return boolean output\n",
    "    plt.contourf(X, Y, z, cmap='YlOrBr')\n",
    "\n",
    "    # add annotated points to show where the boolean inputs lie on the graph\n",
    "    ax.scatter(featureData[:, 0], featureData[:, 1], color='b', alpha=0.5)\n",
    "    for i, txt in enumerate(labelData):\n",
    "        ax.annotate(np.asscalar(txt), (featureData[i, 0], featureData[i, 1]))\n",
    "\n",
    "        # adjust the axis & add labels for the graph\n",
    "    plt.axis([xmin, xmax, ymin, ymax])\n",
    "    plt.ylabel('x2')\n",
    "    plt.xlabel('x1')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show2dFunctionOutput(model_function, featureData, labelData, title):\n",
    "    \"\"\"display results of arbitrary model function on 2-input (x1,x2) , 1-output (z) graphs\"\"\"\n",
    "    # cutoff = 0.5  # 0.5 for Sigmoid. 0.0 for TANH\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    xmin, xmax = np.min(featureData[:, 0]) - 0.1, np.max(featureData[:, 0]) + 0.1\n",
    "    ymin, ymax = np.min(featureData[:, 1]) - 0.1, np.max(featureData[:, 1]) + 0.1\n",
    "\n",
    "    # Create filled countour map to color both sides of the boundary\n",
    "    x = np.linspace(xmin, xmax, 200)\n",
    "    y = np.linspace(ymin, ymax, 200)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    grid = np.c_[X.ravel(), Y.ravel()]\n",
    "    outputs, _, _, _ = model_function(grid)  # get predictions\n",
    "    z = outputs.reshape(X.shape)  # reshape predictions for 2d representation\n",
    "    plt.contourf(X, Y, z, cmap='YlOrBr')\n",
    "\n",
    "    # add annotated points to show where the boolean inputs lie on the graph\n",
    "    ax.scatter(featureData[:, 0], featureData[:, 1], color='b', alpha=0.5)\n",
    "    for i, txt in enumerate(labelData):\n",
    "        ax.annotate(np.asscalar(txt), (featureData[i, 0], featureData[i, 1]))\n",
    "\n",
    "        # adjust the axis & add labels for the graph\n",
    "    plt.axis([xmin, xmax, ymin, ymax])\n",
    "    plt.ylabel('x2')\n",
    "    plt.xlabel('x1')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_output_data(gate='XOR'):\n",
    "    \"\"\" Two dimensional inputs for logic gates\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gate : str\n",
    "        Must be either AND, OR, XOR\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X : array-like, shape(samples, features)\n",
    "        Two dim input for logic gates\n",
    "\n",
    "    truth[gate] : array-like, shapes(samples, )\n",
    "        The truth value for this logic gate\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    X = np.array([[0., 0.],\n",
    "                  [0., 1.],\n",
    "                  [1., 0.],\n",
    "                  [1., 1.]])\n",
    "\n",
    "    truth = {\n",
    "        'AND': np.array([0, 0, 0, 1]),\n",
    "        'OR': np.array([0, 1, 1, 1]),\n",
    "        'XOR': np.array([0, 1, 1, 0])\n",
    "    }\n",
    "\n",
    "    return X, truth[gate][:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ANN_model(input_width = 2,\n",
    "                   layer_widths = [2,1],\n",
    "                   layer_activiations = [sigmoid,sigmoid],\n",
    "                   batchNormalization = False,\n",
    "                  glorot = False):\n",
    "    model = ANN()\n",
    "    print(list(zip(layer_widths,layer_activiations)))\n",
    "    model.set_input_width(input_width)\n",
    "    for lnum,(layerWidth,layerActivation) in enumerate(zip(layer_widths,layer_activiations)):\n",
    "        model.add_layer(nodecount = layerWidth, activation_fcn=layerActivation)\n",
    "    model.initialize(glorot=glorot, batchNormalization=batchNormalization)\n",
    "    return model\n",
    "        \n",
    " \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ANN_model(model=None,\n",
    "                    trainX = None, trainY = None,\n",
    "                    valX=None,valY=None,\n",
    "                    learning_rate=1.0,\n",
    "                    lr_decay=0.999,\n",
    "                    batchsize = 1,\n",
    "                    momentum = 0,\n",
    "                    valPatience=0,\n",
    "                    maxEpochs = 100):\n",
    "    \n",
    "    \n",
    "\n",
    "    preds = model.predict(valX)\n",
    "    correct = valY == preds\n",
    "    print(\"BEFORE TRAINING (randomized weights)\")\n",
    "    model.display_params()\n",
    "    tng_loss_history,val_loss_history = model.fit(tngInputs=trainX, tngTargets=trainY, valInputs=valX, valTargets = valY,\n",
    "                                                  learningRate=learning_rate, learningRateDecay=lr_decay, batchsize=batchsize,\n",
    "                                                  momentum = momentum, valPatience=valPatience,\n",
    "                                                  tolerance=1e-1, maxEpochs=maxEpochs)\n",
    "\n",
    "    preds = model.predict(valX)\n",
    "    correct = valY == preds\n",
    "\n",
    "    print(\"AFTER TRAINING (learned model weights)\")\n",
    "    model.display_params()\n",
    "\n",
    "    show2dFunctionOutput(model.forwardPropagation, X, Y, \"Raw Response of Network\")\n",
    "    makeDecisionBoundaryBool2(model, X, Y, \"XOR predictions from Network\")\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(tng_loss_history,'r')\n",
    "    plt.plot(val_loss_history, 'b')\n",
    "    plt.xlabel(\"iteration\")\n",
    "    plt.ylabel(\"Loss (mse)\")\n",
    "    plt.title(\"Loss over iterations\")\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple network without Batch normalization\n",
    "this is a 2,1 fully connected sigmoid network.  works some of the time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = make_ANN_model(input_width = 2,\n",
    "                       layer_widths = [2,1],  #number of nodes in each layer\n",
    "                       layer_activiations = [sigmoid,sigmoid],\n",
    "                       batchNormalization=False,\n",
    "                       glorot=False)  #activations at each layer\n",
    "\n",
    "print(\"empty model info\")\n",
    "model.summary()\n",
    "\n",
    "X, Y = get_input_output_data(gate='XOR')\n",
    "\n",
    "trainX = X\n",
    "trainY = Y\n",
    "valX = X\n",
    "valY = Y\n",
    "\n",
    "train_ANN_model(model,trainX,trainY,valX,valY,\n",
    "                learning_rate=1.0,lr_decay=0.999, batchsize=4, \n",
    "                momentum = 0, valPatience = 0, maxEpochs = 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple network with batch normalization\n",
    "this is a 2,1 fully connected sigmoid network.  works some of the time...\n",
    "dont forget you need batchsize > 1 for this to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = make_ANN_model(input_width = 2,\n",
    "                       layer_widths = [2,1],  #number of nodes in each layer\n",
    "                       layer_activiations = [sigmoid,sigmoid],\n",
    "                       batchNormalization=True,\n",
    "                       glorot=False)  #activations at each layer\n",
    "\n",
    "print(\"empty model info\")\n",
    "model.summary()\n",
    "\n",
    "X, Y = get_input_output_data(gate='XOR')\n",
    "\n",
    "trainX = X\n",
    "trainY = Y\n",
    "valX = X\n",
    "valY = Y\n",
    "\n",
    "train_ANN_model(model,trainX,trainY,valX,valY,\n",
    "                learning_rate=1.0,lr_decay=0.999, batchsize=4, \n",
    "                momentum = 0, valPatience = 0, maxEpochs = 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple network with Glorot initialization\n",
    "this is a 2,1 fully connected sigmoid network.  works some of the time...\n",
    "\n",
    "but works more often once glorot initializations are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = make_ANN_model(input_width = 2,\n",
    "                       layer_widths = [2,1],  #number of nodes in each layer\n",
    "                       layer_activiations = [sigmoid,sigmoid],\n",
    "                       glorot=True)  #activations at each layer\n",
    "\n",
    "print(\"empty model info\")\n",
    "model.summary()\n",
    "\n",
    "X, Y = get_input_output_data(gate='XOR')\n",
    "\n",
    "trainX = X\n",
    "trainY = Y\n",
    "valX = X\n",
    "valY = Y\n",
    "\n",
    "train_ANN_model(model,trainX,trainY,valX,valY,\n",
    "                learning_rate=1.0,lr_decay=0.999, batchsize=1, \n",
    "                momentum = 0, valPatience = 0, maxEpochs = 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Network with Momentum\n",
    "this is a 2,1 fully connected sigmoid network. works more often and probably trains quicker..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = make_ANN_model(input_width = 2,\n",
    "                       layer_widths = [2,1],  #number of nodes in each layer\n",
    "                       layer_activiations = [sigmoid,sigmoid],\n",
    "                      glorot=True)  #activations at each layer\n",
    "\n",
    "\n",
    "print(\"empty model info\")\n",
    "model.summary()\n",
    "\n",
    "X, Y = get_input_output_data(gate='XOR')\n",
    "\n",
    "trainX = X\n",
    "trainY = Y\n",
    "valX = X\n",
    "valY = Y\n",
    "\n",
    "train_ANN_model(model,trainX,trainY,valX,valY,\n",
    "                learning_rate=1.0,lr_decay=0.999, batchsize=1, \n",
    "                momentum = 0.7, valPatience = 0, maxEpochs = 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Higher Capacity network\n",
    " a 5-3-1 network should learn easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = make_ANN_model(input_width = 2,\n",
    "                   layer_widths = [5,3,1],  #number of nodes in each layer\n",
    "                   layer_activiations = [sigmoid,sigmoid,sigmoid])  #activations at each layer\n",
    "\n",
    "print(\"empty model info\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X, Y = get_input_output_data(gate='XOR')\n",
    "\n",
    "trainX = X\n",
    "trainY = Y\n",
    "valX = X\n",
    "valY = Y\n",
    "\n",
    "train_ANN_model(model,trainX,trainY,valX,valY,\n",
    "                learning_rate=0.5,lr_decay=1.0, batchsize=1, \n",
    "                valPatience = 0, maxEpochs = 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation\n",
    "Create augmented training dataset\n",
    "\n",
    "Add small amounts of random gaussian noise to X datapoints, but keep the same Y labels\n",
    "\n",
    "Probably trains very quickly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateAugmentedTrainData(X,Y,augCount=1000):\n",
    "    \"\"\"starting with original datapoints X and labels Y, return new augmented datapoints of size augCount\"\"\"\n",
    "    xtrain = X #placeholder\n",
    "    ytrain = Y #placeholder\n",
    "    \n",
    "    ##### STUDENT CODE FOR DATA AUGMENTATION #############\n",
    "    # make a larger dataset in which you \n",
    "    # add a small random perturbation to the features of each datapoint\n",
    "    idx = np.random.randint(X.shape[0], size=augCount)\n",
    "    noise = np.random.normal(scale = 0.1, size = (augCount,X.shape[1]))\n",
    "    xtrain = X[idx]+noise\n",
    "    ytrain = Y[idx]\n",
    "    ############# END STUDENT CODE #######################\n",
    "    \n",
    "    return xtrain,ytrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the previous 5-3-1 network, but reinit the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainX,trainY = generateAugmentedTrainData(X,Y,400)\n",
    "\n",
    "valX,valY=X,Y\n",
    "\n",
    "showDatapoints2d(trainX, trainY, title=\"Augmented Training Datapoints\")\n",
    "showDatapoints2d(valX, valY, title=\"Validation Datapoints\")\n",
    "\n",
    "model.initialize()\n",
    "model.summary()\n",
    "\n",
    "train_ANN_model(model,trainX,trainY,valX,valY,\n",
    "                learning_rate=0.5,lr_decay=0.99, batchsize=1, \n",
    "                valPatience = 0, maxEpochs = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN USING WIEGHT REGULARIZATION  \n",
    "#not working correctly yet?\n",
    "\n",
    "trainX,trainY = X,Y\n",
    "valX,valY=X,Y\n",
    "\n",
    "showDatapoints2d(trainX, trainY, title=\"Training Datapoints\")\n",
    "showDatapoints2d(valX, valY, title=\"Validation Datapoints\")\n",
    "\n",
    "model.initialize()\n",
    "model.setL2weightNormalization(lam=0.0003)  #only seems to work for lam < 0.0003\n",
    "model.summary()\n",
    "\n",
    "train_ANN_model(model,trainX,trainY,valX,valY,\n",
    "                learning_rate=0.5,lr_decay=0.999, batchsize=1, \n",
    "                valPatience = 0, maxEpochs = 5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
