{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm 6.4 Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Complete Derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1. / (1. + np.exp(-x))\n",
    "\n",
    "\n",
    "def dSigmoid(x):  # derivative of sigmoid\n",
    "    # Return the correct derivative here\n",
    "    return None\n",
    "\n",
    "\n",
    "def relu(z):  # rectified linear unit activation\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "\n",
    "def dRelu(z):\n",
    "    # Return the correct derivative here\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Complete compute_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN:\n",
    "    data = []\n",
    "    layers = []\n",
    "    inputWidth = 1\n",
    "    outputWidth = 1\n",
    "\n",
    "    class Layer:\n",
    "\n",
    "        \"\"\"class defining the elements of an ANN layer\"\"\"\n",
    "\n",
    "        def __init__(self):\n",
    "            self.w = []\n",
    "            self.b = []\n",
    "            self.nodecount = []\n",
    "            self.activation_fcn = []\n",
    "            self.activation_fcn_derivative = []\n",
    "            self.orderNumber = []\n",
    "            self.previous = None  # link to previous layer\n",
    "            self.next = None  # link to next layer\n",
    "\n",
    "        def set_weights(self, w, b):\n",
    "            \"\"\"set the weights and bias for the layer.  Layer weights should have dimesion: (thislayer_nodecount, previouslayer_nodecount)\n",
    "            the dimension of the bias should be (thislayer_nodecount,1)\"\"\"\n",
    "            self.w = w\n",
    "            self.b = b\n",
    "            return self\n",
    "\n",
    "        def initialize_weights(self):\n",
    "            assert self.w != []\n",
    "            assert self.b != []\n",
    "            self.b = np.random.normal(size=(self.w.shape))  # hidden weight matrix [rows = to, columns = from]\n",
    "            self.w = np.random.normal(size=(self.b.shape))  # hidden biases (column vector)\n",
    "\n",
    "        def set_activation(self, activation_fcn):\n",
    "            self.activation_fcn = activation_fcn\n",
    "            return self\n",
    "\n",
    "        def set_activation_deriv(self, activation_fcn):\n",
    "            if activation_fcn == sigmoid:\n",
    "                self.activation_fcn_derivative = copy.deepcopy(dSigmoid)\n",
    "            elif activation_fcn == relu:\n",
    "                self.activation_fcn_derivative = copy.deepcopy(dRelu)\n",
    "            else:\n",
    "                self.activation_fcn_derivative = None\n",
    "\n",
    "        def compute_pre_activation(self, inputs):\n",
    "            net = np.dot(self.w, inputs) + self.b\n",
    "            return net\n",
    "\n",
    "        def compute_bias_gradient(self, gradient):\n",
    "            g = np.mean(gradient, axis=1)[:, np.newaxis]  # no regularization\n",
    "            return g\n",
    "\n",
    "        def compute_weight_gradient(self, inputs, gradient):\n",
    "            g = np.dot(gradient, inputs.T)\n",
    "            return g\n",
    "\n",
    "        def compute_activation(self, net):\n",
    "            return self.activation_fcn(net)\n",
    "\n",
    "        def compute_activation_derivative(self, net):\n",
    "            return self.activation_fcn_derivative(net)\n",
    "\n",
    "        def compute_activation_gradient(self, net, gradient):\n",
    "            g = np.multiply(gradient, net)\n",
    "            return g\n",
    "\n",
    "        def compute_forward(self, inputs):\n",
    "            \"\"\"Returns layer ouput from input (shape = [nodeCount, input]) of the weighted input plus bias\n",
    "            input shape must be [lastlayer_nodeCount, samples] or [featurecount, samplecount] \"\"\"\n",
    "            net = self.compute_pre_activation(self, inputs)\n",
    "            layer_out = self.compute_activation(net)\n",
    "            return layer_out\n",
    "\n",
    "        def compute_layer_gradients(self, inputs, net, activation, gradient):\n",
    "            \"\"\" computes the loss gradient with respect to desired output of the layer\n",
    "            a set of desired targets is assumed to be matrix of shape [nodecount, samples]: SGD will have [nodecount,1]\n",
    "            hidden_inputs is assumed to be a matrix of shape [hiddenNodeCount, samples]\n",
    "            \n",
    "            This follows algorithm 6.4 line by line in the book!\n",
    "            \n",
    "            \"\"\"\n",
    "            \n",
    "            # f'(a(k))\n",
    "            d_activation = self.compute_activation_derivative(net)  # derivative of sigmoid:  shape = [NodeCount, samples]\n",
    "            \n",
    "            # g <- g * f'(a(k))\n",
    "            g_loss = self.compute_activation_gradient(d_activation, gradient)  # shape = [NodeCount, samples]  [1, 4] for outer layer\n",
    "            \n",
    "            # Delta_b(k) J = g (Take the mean across all 4 samples (batch))\n",
    "            g_loss_b = self.compute_bias_gradient(g_loss)  # mean gradient with respect to BIAS, shape = [NodeCount, 1]\n",
    "            \n",
    "            # Delta w(k) J = g * h(k-1)\n",
    "            g_loss_w = self.compute_weight_gradient(activation, g_loss)  # [1, 3]  Hidden layer outputs after activation\n",
    "            \n",
    "            # g <- W(k).T * g\n",
    "            g_loss_backprop = np.dot(self.w.T, g_loss)  # gradient to propagate back, shape = [hiddenNodeCount,samples]\n",
    "            \n",
    "            return g_loss_w, g_loss_b, g_loss_backprop\n",
    "\n",
    "        def update_Layer(self, weightUpdate, biasUpdate):\n",
    "            self.w = self.w + weightUpdate\n",
    "            self.b = self.b + biasUpdate\n",
    "\n",
    "    def __init__(self):\n",
    "        self.data = []\n",
    "        self.layers = []\n",
    "        self.inputWidth = 1\n",
    "        self.outputWidth = 1\n",
    "\n",
    "    def set_input_width(self, inputWidth):\n",
    "        \"\"\"defines the input layer width for the network\"\"\"\n",
    "        self.inputWidth = inputWidth\n",
    "\n",
    "    def add_layer(self, nodecount=1, activation_fcn=relu):\n",
    "        \"\"\"adds a layer to the neural network and returns the layer\"\"\"\n",
    "        oldLayerCount = len(self.layers)\n",
    "        thislayer = ANN.Layer()\n",
    "        thislayer.orderNumber = oldLayerCount + 1\n",
    "        if oldLayerCount > 0:  # other layers have been added already\n",
    "            lastLayer = self.layers[-1]\n",
    "            thislayer.previous = lastLayer\n",
    "            lastLayer.next = thislayer\n",
    "            layerInputSize = lastLayer.w.shape[1]\n",
    "\n",
    "        else:  # this will be the first layer\n",
    "            layerInputSize = self.inputWidth\n",
    "        thislayer.w = np.zeros((layerInputSize, nodecount))\n",
    "        thislayer.b = np.zeros((1, nodecount))\n",
    "        thislayer.activation_fcn = activation_fcn\n",
    "        thislayer.set_activation_deriv(activation_fcn)\n",
    "        self.layers = self.layers + [thislayer]\n",
    "        return thislayer\n",
    "\n",
    "    def forwardPropagation(self, inputs):\n",
    "        \"\"\"Compute forward pass of two layer network\n",
    "        inputs are assumed to be (shape=[sampleCount,featureCount])\n",
    "        returns a matrix of raw outputs with one row of output per node (shape=[sampleCount, outputNodeCount])\n",
    "        Internal matrices are shaped for efficiency to avoid internal transposes (columns hold observations/samples) \"\"\"\n",
    "\n",
    "        # inputs and outputs will be transposed for efficiency during forwardPropagation and untransposed before returning\n",
    "\n",
    "        nets = []\n",
    "        activations = []\n",
    "        layer_input = inputs.T\n",
    "\n",
    "        for lnum, layer in enumerate(self.layers):\n",
    "            # inputs = inputs + inputs\n",
    "            layer_net = layer.compute_pre_activation(layer_input)\n",
    "            nets.append(layer_net)\n",
    "\n",
    "            layer_out = layer.compute_activation(layer_net)\n",
    "            activations.append(layer_out)\n",
    "\n",
    "            layer_input = layer_out\n",
    "        raw_output = layer_out.T\n",
    "        return raw_output, inputs, nets, activations\n",
    "\n",
    "    def backPropagation(self, inputs, desiredOutputs, learningRate):\n",
    "        w_grads = []\n",
    "        b_grads = []\n",
    "        # store nets and activations for each layer\n",
    "        raw_output, _, nets, activations = self.forwardPropagation(inputs)\n",
    "        layer_desired_out = desiredOutputs\n",
    "\n",
    "        # Note: This is only part of the gradient\n",
    "        layer_grad = desiredOutputs - raw_output\n",
    "\n",
    "        #  computation of full gradient handled inside the loop below\n",
    "        for lnum, layer in reversed(list(enumerate(self.layers))):\n",
    "            if lnum == 0:\n",
    "                prev_layer_output = inputs.T\n",
    "            else:\n",
    "                prev_layer_output = activations[lnum - 1]\n",
    "            if lnum == 1:\n",
    "                w_grad, b_grad, loss_grad = layer.compute_layer_gradients(inputs[lnum], nets[lnum], prev_layer_output,\n",
    "                                                                          layer_grad.T)\n",
    "            else:\n",
    "                w_grad, b_grad, loss_grad = layer.compute_layer_gradients(inputs[lnum], nets[lnum], prev_layer_output,\n",
    "                                                                          layer_grad)\n",
    "            layer.update_Layer(w_grad * learningRate, b_grad * learningRate)\n",
    "            layer_grad = loss_grad\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        \"\"\"Compute predictions using forward propagation for single binary classification at threshold\n",
    "        X is a standard dataFrame without biases (shape=[observationCount,featureCount])\n",
    "        returns a standard column vector of binary predictions in {0,1}: (shape=[observationCount, 1])\"\"\"\n",
    "        raw_predictions, net_inputs, net_lst, activation_lst = self.forwardPropagation(X)\n",
    "        preds = raw_predictions > threshold\n",
    "        return preds\n",
    "\n",
    "    def compute_loss(self, inputs, desired_targets):\n",
    "        \"\"\"computes the (scalar) loss using MSE of a set of targets and sigmoid outputs\n",
    "        inputs is assumed to be a matrix of shape [samples, features]\n",
    "         desired_targets is assumed to be a matrix of shape [samples, 1]\"\"\"\n",
    "        raw_outputs = self.forwardPropagation(inputs)[0]\n",
    "        #################################################################################\n",
    "        # Complete the MSE loss\n",
    "        \n",
    "        \n",
    "        return None\n",
    "        \n",
    "        #################################################################################\n",
    "        \n",
    "\n",
    "    def fit(self, inputs, desired_targets, learningRate, learningRateDecay, tolerance=1e-2):\n",
    "        done = False\n",
    "        loss_history = []\n",
    "        print(\"Training Model...\")\n",
    "\n",
    "        while not done:\n",
    "            learningRate = learningRate * learningRateDecay\n",
    "            preds = self.predict(inputs)\n",
    "            correct = desired_targets == preds\n",
    "            curr_loss = np.asscalar(self.compute_loss(inputs, desired_targets))\n",
    "            loss_history.append(curr_loss)\n",
    "            if curr_loss < tolerance:\n",
    "                done = True\n",
    "            # run an epoch of backprop\n",
    "            self.backPropagation(inputs, desired_targets, learningRate=learningRate)\n",
    "        print(\"Training Complete!\")\n",
    "\n",
    "        return loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeDecisionBoundaryBool2(model, featureData, labelData, title):\n",
    "    '''Build decision boundary figrue for 2-input, 1-output boolean logic functions\n",
    "    Note that this assumes a hard sigmoid was used and establishes a cutoff at 0.5\n",
    "    for predicting 0 or 1'''\n",
    "    cutoff = 0.5  # 0.5 for Sigmoid. 0.0 for TANH\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    xmin, xmax = np.min(featureData[:, 0]) - 0.1, np.max(featureData[:, 0]) + 0.1\n",
    "    ymin, ymax = np.min(featureData[:, 1]) - 0.1, np.max(featureData[:, 1]) + 0.1\n",
    "\n",
    "    # Create filled countour map to color both sides of the boundary\n",
    "    x = np.linspace(xmin, xmax, 200)\n",
    "    y = np.linspace(ymin, ymax, 200)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    grid = np.c_[X.ravel(), Y.ravel()]\n",
    "    preds = model.predict(grid)  # get predictions\n",
    "    z = preds.reshape(X.shape) > cutoff  # cutoff on predictions to return boolean output\n",
    "    plt.contourf(X, Y, z, cmap='YlOrBr')\n",
    "\n",
    "    # add annotated points to show where the boolean inputs lie on the graph\n",
    "    ax.scatter(featureData[:, 0], featureData[:, 1], color='b', alpha=0.5)\n",
    "    for i, txt in enumerate(labelData):\n",
    "        ax.annotate(np.asscalar(txt), (featureData[i, 0], featureData[i, 1]))\n",
    "\n",
    "        # adjust the axis & add labels for the graph\n",
    "    plt.axis([xmin, xmax, ymin, ymax])\n",
    "    plt.ylabel('x2')\n",
    "    plt.xlabel('x1')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show2dFunctionOutput(model_function, featureData, labelData, title):\n",
    "    \"\"\"display results of arbitrary model function on 2-input (x1,x2) , 1-output (z) graphs\"\"\"\n",
    "    # cutoff = 0.5  # 0.5 for Sigmoid. 0.0 for TANH\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    xmin, xmax = np.min(featureData[:, 0]) - 0.1, np.max(featureData[:, 0]) + 0.1\n",
    "    ymin, ymax = np.min(featureData[:, 1]) - 0.1, np.max(featureData[:, 1]) + 0.1\n",
    "\n",
    "    # Create filled countour map to color both sides of the boundary\n",
    "    x = np.linspace(xmin, xmax, 200)\n",
    "    y = np.linspace(ymin, ymax, 200)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    grid = np.c_[X.ravel(), Y.ravel()]\n",
    "    outputs, _, _, _ = model_function(grid)  # get predictions\n",
    "    z = outputs.reshape(X.shape)  # reshape predictions for 2d representation\n",
    "    plt.contourf(X, Y, z, cmap='YlOrBr')\n",
    "\n",
    "    # add annotated points to show where the boolean inputs lie on the graph\n",
    "    ax.scatter(featureData[:, 0], featureData[:, 1], color='b', alpha=0.5)\n",
    "    for i, txt in enumerate(labelData):\n",
    "        ax.annotate(np.asscalar(txt), (featureData[i, 0], featureData[i, 1]))\n",
    "\n",
    "        # adjust the axis & add labels for the graph\n",
    "    plt.axis([xmin, xmax, ymin, ymax])\n",
    "    plt.ylabel('x2')\n",
    "    plt.xlabel('x1')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_output_data(gate='XOR'):\n",
    "    \"\"\" Two dimensional inputs for logic gates\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gate : str\n",
    "        Must be either AND, OR, XOR\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X : array-like, shape(samples, features)\n",
    "        Two dim input for logic gates\n",
    "\n",
    "    truth[gate] : array-like, shapes(samples, )\n",
    "        The truth value for this logic gate\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    X = np.array([[0., 0.],\n",
    "                  [0., 1.],\n",
    "                  [1., 0.],\n",
    "                  [1., 1.]])\n",
    "\n",
    "    truth = {\n",
    "        'AND': np.array([0, 0, 0, 1]),\n",
    "        'OR': np.array([0, 1, 1, 1]),\n",
    "        'XOR': np.array([0, 1, 1, 0])\n",
    "    }\n",
    "\n",
    "    return X, truth[gate][:, np.newaxis]\n",
    "\n",
    "def train_ANN_model(hidden_nodes=3,\n",
    "                    learning_rate=1.0,\n",
    "                    lr_decay=0.999,\n",
    "                    hidden_act=sigmoid,\n",
    "                    output_act=sigmoid,\n",
    "                   gate_type='XOR'):\n",
    "    \n",
    "    X, Y = get_input_output_data(gate=gate_type)\n",
    "\n",
    "    model = ANN()\n",
    "    model.set_input_width(X.shape[1])\n",
    "    l1_weights = np.random.rand(hidden_nodes, 2)\n",
    "    l1_bias = np.random.rand(hidden_nodes, 1)\n",
    "    l2_weights = np.random.rand(1, hidden_nodes)\n",
    "    l2_bias = np.random.rand(1, 1)\n",
    "\n",
    "    layer1 = model.add_layer(nodecount=hidden_nodes, activation_fcn=hidden_act)\n",
    "    layer1.set_weights(l1_weights, l1_bias)\n",
    "    layer2 = model.add_layer(nodecount=1, activation_fcn=output_act)\n",
    "    layer2.set_weights(l2_weights, l2_bias)\n",
    "\n",
    "    preds = model.predict(X)\n",
    "    correct = Y == preds\n",
    "\n",
    "    print(\"predictions: \\n\", preds)\n",
    "    print(\"Predictions correct?: \\n\", correct)\n",
    "\n",
    "    loss_history = model.fit(inputs=X, desired_targets=Y, learningRate=learning_rate, learningRateDecay=lr_decay,\n",
    "                             tolerance=1e-1)\n",
    "\n",
    "    preds = model.predict(X)\n",
    "    correct = Y == preds\n",
    "\n",
    "    print(\"predictions: \\n\", preds)\n",
    "    print(\"Predictions correct?: \\n\", correct)\n",
    "\n",
    "    show2dFunctionOutput(model.forwardPropagation, X, Y, \"Sigmoid Response of Network - student\")\n",
    "    makeDecisionBoundaryBool2(model, X, Y, \"XOR predictions - student\")\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(loss_history)\n",
    "    plt.xlabel(\"iteration\")\n",
    "    plt.ylabel(\"Loss (mse)\")\n",
    "    plt.title(\"Loss over iterations\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_ANN_model(hidden_nodes=3,\n",
    "               learning_rate=1.0,\n",
    "               lr_decay=0.999,\n",
    "               hidden_act=sigmoid,\n",
    "               output_act=sigmoid,\n",
    "               gate_type='XOR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
