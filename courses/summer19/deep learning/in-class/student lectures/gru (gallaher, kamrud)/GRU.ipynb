{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding a Single Gated Recurrent Unit (GRU)\n",
    "\n",
    "## References: I HIGHLY recommend you check these out, they make the concepts easily understood in about 5 minutes\n",
    "\n",
    "### Walks through creating a GRU step by step, the author makes it super super simple to understand\n",
    "https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be\n",
    "\n",
    "### This video does an AMAZING job of helping you to understand what's going on with an LSTM and a GRU in a conceptual way using no math\n",
    "https://www.youtube.com/watch?v=8HyCNIVRbSU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRUs solve the vanishing gradient problem of a standard RNN through the use of update and reset gates. These gates decide what information needs to be remembered and what should be forgotten.\n",
    "\n",
    "### Here is a recurrent neural network that uses GRUs:\n",
    "![gru rnn](overview.jpg)\n",
    "\n",
    "### To understand what's going on, we're going to examine a single GRU:\n",
    "![arrows](arrows.jpg)\n",
    "\n",
    "#### We are using the following notations:\n",
    "![notations](notations.jpg)\n",
    "\n",
    "#### Additionally, $x_t$ is the current input, $h_{t-1}$ is the past state's \"hidden state\", and $h_t$ is the information that we are going to pass along to the next GRU, or the \"hidden state.\"\n",
    "\n",
    "#### I know this looks supppper complicated, but I promise it's actually simple. Let's examine it step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup code\n",
    "\n",
    "# seed np\n",
    "np.random.seed( 10 )\n",
    "\n",
    "# The input, x_t, a simple 3x3 array\n",
    "\n",
    "x = np.random.random( size=( 3, 3 ) )\n",
    "\n",
    "# The previous hidden state, h_{t-1}\n",
    "\n",
    "h_prev = np.random.random( size=( 3, 3 ) )\n",
    "\n",
    "# The input weights for the update gate\n",
    "\n",
    "w_z = np.random.random( size=( 3, 3 ) )\n",
    "\n",
    "# The previous hidden state's weights for the update gate\n",
    "\n",
    "u_z = np.random.random( size=( 3, 3 ) )\n",
    "\n",
    "# The input weights for the reset gate\n",
    "\n",
    "w_r = np.random.random( size=( 3, 3 ) )\n",
    "\n",
    "# The previous hidden state's weights for the reset gate\n",
    "\n",
    "u_r = np.random.random( size=( 3, 3 ) )\n",
    "\n",
    "# The input weights\n",
    "\n",
    "w = np.random.random( size=( 3, 3 ) )\n",
    "\n",
    "# The previous hidden state's weights\n",
    "\n",
    "u = np.random.random( size=( 3, 3 ) )\n",
    "\n",
    "def sigmoid( x ):\n",
    "    \n",
    "    return ( 1 / ( 1 + math.e ** -x ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: The update gate\n",
    "## The update gate helps the model to determine how much of the past information needs to be passed along to the future.\n",
    "\n",
    "![1](1.jpg)\n",
    "\n",
    "### We start with calculating the update gate, $z_t$, for the current time step, t, using the formula:\n",
    "# $z_t = \\sigma ( W^{ (z) } x_t + U^{ (z) } h_{t-1} )$\n",
    "\n",
    "##### When the current input, $x_t$ is plugged into the GRU, it is multiplied by its own weight, $W_z$. \n",
    "##### The same occurs for the previous GRU's hidden state, $h_{t-1}$: it gets multiplied by its own weight, $U(z)$.\n",
    "##### The results of the two above operations are summed together, and then a sigmoid activation function is applied to squash the results between 0 and 1.\n",
    "\n",
    "#### We will see how the update gate, $z_t$, gets used later. For now, let's calculate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update gate calculations\n",
    "\n",
    "# Input - x\n",
    "# Input update weights - w_z\n",
    "# Previous hidden state - h_prev\n",
    "# Previous hidden state update weights - u_z\n",
    "# Sigmoid function - sigmoid( x ) returns the sigmoid of x\n",
    "\n",
    "\"\"\" \n",
    "\n",
    "Using np.multiply gives you the result of an elementwise multiplication, or:\n",
    "\n",
    "a = [ 1, 2 ]\n",
    "    [ 3, 4 ]\n",
    "    \n",
    "b = [ 4, 5 ]\n",
    "    [ 6, 7 ]\n",
    "    \n",
    "np.multiply( a, b ) = \n",
    "\n",
    "    [ ( 1 * 4 ), ( 2 * 5 ) ]\n",
    "    [ ( 3 * 6 ), ( 4 * 7 ) ]\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "# *******************************************************************\n",
    "# STUDENT CODE HERE\n",
    "# *******************************************************************\n",
    "\n",
    "# Replace 0s with correct forumula\n",
    "\n",
    "# Input times the update input weights\n",
    "# Input - x\n",
    "# Input update weights - w_z\n",
    "weighted_update_input = 0\n",
    "\n",
    "\n",
    "# Previous hiden state times the update previous state weights\n",
    "# Previous hidden state - h_prev\n",
    "# Previous hidden state update weights - u_z\n",
    "weighted_update_prev_hidden = 0\n",
    "\n",
    "# Sum the above two\n",
    "sum = 0\n",
    "\n",
    "# Take the sigmoid function of the sum\n",
    "# sigmoid( x ) returns the sigmoid of x\n",
    "z = 0\n",
    "\n",
    "\n",
    "# *******************************************************************\n",
    "# END STUDENT CODE\n",
    "# *******************************************************************\n",
    "\n",
    "\n",
    "# This is the what comes out of the update gate\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: The reset gate\n",
    "## The reset gate is used to decide how much of the past information to forget.\n",
    "\n",
    "![2](2.jpg)\n",
    "\n",
    "### Next, we need to decide what information we need to forget, or $r_t$. The formula is below\n",
    "# $r_t = \\sigma ( W^{ (r) } x_t + U^{ (r) } h_{t-1} )$\n",
    "\n",
    "##### This formula is the same as the update gate, but the difference is that we use a different set of weights, $r$. Additionally, the reset gate will be used differently that the update gate.\n",
    "##### The results of the two above operations are summed together, and then a sigmoid activation function is applied to squash the results between 0 and 1.\n",
    "\n",
    "#### We will see how the update gate, $r_t$, gets used later. For now, let's calculate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reset gate calculations\n",
    "\n",
    "# Input - x\n",
    "# Input reset weights - w_r\n",
    "# Previous hidden state - h_prev\n",
    "# Previous hidden state reset weights - u_r\n",
    "# Sigmoid function - sigmoid( x ) returns the sigmoid of x\n",
    "\n",
    "# Using np.multiply gives you the result of an elementwise multiplication\n",
    "\n",
    "# *******************************************************************\n",
    "# STUDENT CODE HERE\n",
    "# *******************************************************************\n",
    "\n",
    "# Input times the reset input weights\n",
    "# Input - x\n",
    "# Input reset weights - w_r\n",
    "weighted_reset_input = 0\n",
    "\n",
    "# Previous hiden state times the reset previous state weights\n",
    "# Previous hidden state - h_prev\n",
    "# Previous hidden state reset weights - u_r\n",
    "weighted_reset_prev_hidden = 0\n",
    "\n",
    "# Sum the above two\n",
    "sum = 0\n",
    "\n",
    "# Take the sigmoid function of the sum\n",
    "r = 0\n",
    "\n",
    "# *******************************************************************\n",
    "# END STUDENT CODE\n",
    "# *******************************************************************\n",
    "\n",
    "# This is what comes out of the reset gate\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Updating the current input with what we need to remember (update gate) and what we need to forget (reset gate)\n",
    "\n",
    "\n",
    "![3](3.jpg)\n",
    "\n",
    "### Now we need to decide how what we need to remember and what we need to forget from the past affects our current input:\n",
    "# $h'_t = tanh( W  x_t + r_t \\odot U  h_{t-1})$\n",
    "\n",
    "##### We accomplish the above by doing four steps:\n",
    "   1. Multiply the input $x_t$ with a weight $W$ and $h_{t-1}$ with a weight $U$.\n",
    "   2. Calculate the Hadamard (element-wise) product between the reset gate, $r_t$ and $U  h_{t-1}$. This determines what to remove from the previous steps.\n",
    "   3. Sum steps 1 and 2.\n",
    "   4. Apply a tanh function to 3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hidden state update\n",
    "\n",
    "# Input - x\n",
    "# Input weights - w\n",
    "# Previous hidden state - h_prev\n",
    "# Previous hidden state weights - u\n",
    "# Reset gate info - r\n",
    "# np.tanh( x ) returns the tanh of x\n",
    "\n",
    "# Using np.multiply gives you the result of an elementwise multiplication\n",
    "\n",
    "# *******************************************************************\n",
    "# STUDENT CODE HERE\n",
    "# *******************************************************************\n",
    "\n",
    "# Step 1\n",
    "\n",
    "# The input times the input weights\n",
    "# Input - x\n",
    "# Input weights - w\n",
    "weighted_input = 0\n",
    "\n",
    "# The previous state times the previous hidden state weights\n",
    "# Previous hidden state - h_prev\n",
    "# Previous hidden state weights - u\n",
    "weighted_prev_hidden = 0\n",
    "\n",
    "# Step 2\n",
    "\n",
    "# Element wise multiplication of the reset gate and the weighted previous hidden state\n",
    "# Reset gate info - r\n",
    "hadamardProduct = 0\n",
    "\n",
    "# Step 3\n",
    "# Sum the weighted input and step 2\n",
    "sum = 0\n",
    "\n",
    "# Step 4\n",
    "# Take the tanh of the sum\n",
    "# np.tanh( x ) returns the tanh of x\n",
    "hPrime = 0\n",
    "\n",
    "# *******************************************************************\n",
    "# END STUDENT CODE\n",
    "# *******************************************************************\n",
    "\n",
    "# This array tells us what is important to remember and what we can forget\n",
    "hPrime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Output the new hidden state\n",
    "## This new hidden state, $h_t$, contains the information that we've just calculated about what is important to remember and what can be forgotten. It becomes the next GRU's $h_{t-1}$.\n",
    "\n",
    "![4](4.jpg)\n",
    "\n",
    "### Let's calculate what information to pass onto the next GRU:\n",
    "# $h_t = z_t \\odot h_{t-1} + ( 1 - z_t ) \\odot h'_t $\n",
    "\n",
    "##### We accomplish the above by doing three steps:\n",
    "   1. Apply an element wise multiplication to the update gate, $z_t$, and the previous hidden state, $h_{t-1}$.\n",
    "   2. Apply an element wise multiplication to $(1 - z_t)$ and $h'_t$.\n",
    "   3. Sum the results of 1 and 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output new hidden state\n",
    "\n",
    "# Previous hidden state - h_prev\n",
    "# Previous hidden state weights - u\n",
    "# Update gate info - z\n",
    "# h' - hPrime\n",
    "\n",
    "# Using np.multiply gives you the result of an elementwise multiplication\n",
    "\n",
    "# *******************************************************************\n",
    "# STUDENT CODE HERE\n",
    "# *******************************************************************\n",
    "\n",
    "# Step 1\n",
    "# Element wise multiplication of the update gate z and the previous hidden state\n",
    "# Update gate info - z\n",
    "# Previous hidden state - h_prev\n",
    "step1 = 0\n",
    "\n",
    "# Step 2\n",
    "# Element wise multiplication of ( 1 - z) and the h prime\n",
    "# h prime - h'\n",
    "step2 = 0\n",
    "\n",
    "# Step 3\n",
    "# Sum steps 1 and 2 \n",
    "newHiddenState = 0\n",
    "\n",
    "# *******************************************************************\n",
    "# END STUDENT CODE\n",
    "# *******************************************************************\n",
    "\n",
    "# This is the information that we pass onto the next GRU.\n",
    "# It contains the relevant information about what's important and what we can forget\n",
    "newHiddenState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
