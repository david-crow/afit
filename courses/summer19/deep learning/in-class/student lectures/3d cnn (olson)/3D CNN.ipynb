{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import tensorflow as tf\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D # for 3d plotting\n",
    "import os\n",
    "\n",
    "import h5py\n",
    "import itertools as it\n",
    "import datetime\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_raw shape:  (10000, 4096)\n",
      "y_train_raw shape:  (10000,)\n",
      "x_test_raw shape:   (2000, 4096)\n",
      "y_test_raw shape:   (2000,)\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "with h5py.File(os.getcwd() + '\\\\input\\\\full_dataset_vectors.h5', 'r') as hf:\n",
    "    x_train_raw = hf[\"X_train\"][:]\n",
    "    y_train_raw = hf[\"y_train\"][:]\n",
    "    x_test_raw = hf[\"X_test\"][:]\n",
    "    y_test_raw = hf[\"y_test\"][:]\n",
    "\n",
    "\n",
    "# length check\n",
    "assert(len(x_train_raw) == len(y_train_raw))\n",
    "assert(len(x_test_raw) == len(y_test_raw))\n",
    "\n",
    "print (\"x_train_raw shape: \", x_train_raw.shape)\n",
    "print (\"y_train_raw shape: \", y_train_raw.shape)\n",
    "\n",
    "print (\"x_test_raw shape:  \", x_test_raw.shape)\n",
    "print (\"y_test_raw shape:  \", y_test_raw.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to RGB values\n",
    "Now, lets implement a 3D convolutional Neural network on this dataset. To use 2D convolutions, we first convert every image into a 3D shape : width, height, channels. Channels represents the slices of Red, Green, and Blue layers. So it is set as 3. In the similar manner, we will convert the input dataset into 4D shape in order to use 3D convolution for : length, breadth, height, channel (r/g/b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1D vector to rgb values, provided by ../input/plot3d.py\n",
    "def array_to_color(array, cmap=\"Oranges\"):\n",
    "    s_m = plt.cm.ScalarMappable(cmap=cmap)\n",
    "    return s_m.to_rgba(array)[:,:-1]\n",
    "\n",
    "# Transform data from 1d to 3d rgb\n",
    "def rgb_data_transform(data):\n",
    "    data_t = []\n",
    "    for i in range(data.shape[0]):\n",
    "        data_t.append(array_to_color(data[i]).reshape(16, 16, 16, 3))\n",
    "    return np.asarray(data_t, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 10 # from 0 to 9, 10 labels totally\n",
    "\n",
    "x_train = rgb_data_transform(x_train_raw)\n",
    "x_test = rgb_data_transform(x_test_raw)\n",
    "\n",
    "## convert target variable into one-hot\n",
    "y_train = to_categorical(y_train_raw, n_classes)\n",
    "y_test = to_categorical(y_test_raw, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape:  (10000, 16, 16, 16, 3)\n",
      "y_train shape:  (10000, 10)\n",
      "x_test shape:   (2000, 16, 16, 16, 3)\n",
      "y_test shape:   (2000, 10)\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('inputs'):\n",
    "    x_input = tf.placeholder(tf.float32, shape=[None, 16, 16, 16, 3])\n",
    "    y_input = tf.placeholder(tf.float32, shape=[None, n_classes]) \n",
    "\n",
    "print (\"x_train shape: \", x_train.shape)\n",
    "print (\"y_train shape: \", y_train.shape)\n",
    "\n",
    "print (\"x_test shape:  \", x_test.shape)\n",
    "print (\"y_test shape:  \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model(x_train_data, keep_rate=0.5, seed=None, kernel_size_1=5, kernel_size_2=3, pool_size=3, \n",
    "              dense_activation1='relu', dense_activation2='relu'):\n",
    "    \n",
    "    with tf.name_scope(\"layer_a\"):\n",
    "        # conv => 16*16*16\n",
    "        conv1 = tf.layers.conv3d(inputs=x_train_data, filters=16, kernel_size=[kernel_size_1,kernel_size_1,kernel_size_1], padding='same', activation=tf.nn.relu)\n",
    "        # conv => 16*16*16\n",
    "        conv2 = tf.layers.conv3d(inputs=conv1, filters=32, kernel_size=[kernel_size_2,kernel_size_2,kernel_size_2], padding='same', activation=tf.nn.relu)\n",
    "        # pool => 8*8*8\n",
    "        pool3 = tf.layers.max_pooling3d(inputs=conv2, pool_size=[pool_size, pool_size, pool_size], strides=pool_size)\n",
    "        \n",
    "    with tf.name_scope(\"layer_c\"):\n",
    "        # conv => 8*8*8\n",
    "        conv4 = tf.layers.conv3d(inputs=pool3, filters=64, kernel_size=[kernel_size_1,kernel_size_1,kernel_size_1], padding='same', activation=tf.nn.relu)\n",
    "        # conv => 8*8*8\n",
    "        conv5 = tf.layers.conv3d(inputs=conv4, filters=128, kernel_size=[kernel_size_2,kernel_size_2,kernel_size_2], padding='same', activation=tf.nn.relu)\n",
    "        # pool => 4*4*4\n",
    "        pool6 = tf.layers.max_pooling3d(inputs=conv5, pool_size=[pool_size, pool_size, pool_size], strides=pool_size)\n",
    "        \n",
    "    with tf.name_scope(\"batch_norm\"):\n",
    "        cnn3d_bn = tf.layers.batch_normalization(inputs=pool6, training=True)\n",
    "        \n",
    "    with tf.name_scope(\"fully_con\"):\n",
    "        flattening = tf.reshape(cnn3d_bn, [-1, cnn3d_bn.shape[1]*cnn3d_bn.shape[2]*cnn3d_bn.shape[3]*cnn3d_bn.shape[4]])\n",
    "        dense = tf.layers.dense(inputs=flattening, units=2048, activation=dense_activation1)\n",
    "        dense = tf.layers.dense(inputs=flattening, units=512, activation=dense_activation2)\n",
    "        # (1-keep_rate) is the probability that the node will be kept\n",
    "        dropout = tf.layers.dropout(inputs=dense, rate=keep_rate, training=True)\n",
    "        \n",
    "    with tf.name_scope(\"y_conv\"):\n",
    "        y_conv = tf.layers.dense(inputs=dropout, units=10)\n",
    "    \n",
    "    return y_conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define function to train and evaluate NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network(x_train_data, y_train_data, x_test_data, y_test_data, **kwargs):\n",
    "    \n",
    "    learning_rate = kwargs.get('learning_rate',0.05)\n",
    "    keep_rate = kwargs.get('keep_rate',0.7)\n",
    "    epochs = kwargs.get('epochs',100)\n",
    "    batch_size = kwargs.get('batch_size',128)\n",
    "    kernel_size_1 = kwargs.get('kernel_size_1',5)\n",
    "    kernel_size_2 = kwargs.get('kernel_size_2',3)\n",
    "    pool_size = kwargs.get('pool_size',3)\n",
    "    dense_activation1 = kwargs.get('dense_activation1','relu')\n",
    "    dense_activation2 = kwargs.get('dense_activation2','relu')\n",
    "\n",
    "    with tf.name_scope(\"cross_entropy\"):\n",
    "        prediction = cnn_model(x_input, keep_rate, 1, kernel_size_1, kernel_size_2, \n",
    "                               pool_size, dense_activation1, dense_activation2)\n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y_input))\n",
    "                              \n",
    "    with tf.name_scope(\"training\"):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "           \n",
    "    correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y_input, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "    \n",
    "    iterations = int(len(x_train_data)/batch_size) + 1\n",
    "    \n",
    "    print(kwargs)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        import datetime\n",
    "\n",
    "        start_time = datetime.datetime.now()\n",
    "\n",
    "        iterations = int(len(x_train_data)/batch_size) + 1\n",
    "        # run epochs\n",
    "        for epoch in range(epochs):\n",
    "            start_time_epoch = datetime.datetime.now()\n",
    "            epoch_loss = 0\n",
    "            # mini batch\n",
    "            for itr in range(iterations):\n",
    "                mini_batch_x = x_train_data[itr*batch_size: (itr+1)*batch_size]\n",
    "                mini_batch_y = y_train_data[itr*batch_size: (itr+1)*batch_size]\n",
    "                _optimizer, _cost = sess.run([optimizer, cost], feed_dict={x_input: mini_batch_x, y_input: mini_batch_y})\n",
    "                epoch_loss += _cost\n",
    "\n",
    "            #  using mini batch in case not enough memory\n",
    "            acc = 0\n",
    "            itrs = int(len(x_test_data)/batch_size) + 1\n",
    "            for itr in range(itrs):\n",
    "                mini_batch_x_test = x_test_data[itr*batch_size: (itr+1)*batch_size]\n",
    "                mini_batch_y_test = y_test_data[itr*batch_size: (itr+1)*batch_size]\n",
    "                acc += sess.run(accuracy, feed_dict={x_input: mini_batch_x_test, y_input: mini_batch_y_test})\n",
    "            \n",
    "            if(epoch % int(epochs/10) == 0):\n",
    "                print(int((epoch/epochs)*100),'% complete')\n",
    "\n",
    "        end_time = datetime.datetime.now()\n",
    "        print('Testing Set Accuracy:',acc/itrs)\n",
    "        print('Time elapsed: ', str(end_time - start_time))\n",
    "        \n",
    "    return acc/itrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a dict to allow all possible combos of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "#####################\n",
    "# STUDENT CODE HERE #\n",
    "#####################\n",
    "#####################\n",
    "\n",
    "# Prepare the Grid\n",
    "param_grid = dict(learning_rate=[0.0001], \n",
    "                  keep_rate=[0.2],\n",
    "                  epochs=[201],\n",
    "                  batch_size=[128],\n",
    "                  kernel_size_1=[3], \n",
    "                  kernel_size_2=[3], \n",
    "                  pool_size=[2],\n",
    "                  dense_activation1=['relu'],\n",
    "                  dense_activation2=['sigmoid']\n",
    "                  )\n",
    "\n",
    "keys = param_grid.keys()\n",
    "values = (param_grid[key] for key in keys)\n",
    "combinations = [dict(zip(keys, combination)) for combination in it.product(*values)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try all possible combs of hyperparameters and report best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0729 09:47:09.215701  6944 deprecation.py:323] From <ipython-input-6-948e11d448d3>:6: conv3d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv3D` instead.\n",
      "W0729 09:47:09.218691  6944 deprecation.py:506] From C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-- 1 -of- 1 -----------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0729 09:47:09.403327  6944 deprecation.py:323] From <ipython-input-6-948e11d448d3>:10: max_pooling3d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.MaxPooling3D instead.\n",
      "W0729 09:47:09.550773  6944 deprecation.py:323] From <ipython-input-6-948e11d448d3>:21: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
      "W0729 09:47:09.623094  6944 deprecation.py:323] From <ipython-input-6-948e11d448d3>:25: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "W0729 09:47:09.845872  6944 deprecation.py:323] From <ipython-input-6-948e11d448d3>:28: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "W0729 09:47:09.906260  6944 deprecation.py:323] From <ipython-input-7-b8456cb297b9>:16: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.0001, 'keep_rate': 0.2, 'epochs': 201, 'batch_size': 128, 'kernel_size_1': 3, 'kernel_size_2': 3, 'pool_size': 2, 'dense_activation1': 'relu', 'dense_activation2': 'sigmoid'}\n",
      "0 % complete\n",
      "9 % complete\n",
      "19 % complete\n",
      "29 % complete\n",
      "39 % complete\n",
      "49 % complete\n",
      "59 % complete\n",
      "69 % complete\n",
      "79 % complete\n",
      "89 % complete\n",
      "99 % complete\n",
      "Testing Set Accuracy: 0.47999998927116394\n",
      "Time elapsed:  0:01:02.799469\n",
      "Index - 0\n",
      "Value - 0.47999998927116394\n",
      "Hyper-parameters {'learning_rate': 0.0001, 'keep_rate': 0.2, 'epochs': 201, 'batch_size': 128, 'kernel_size_1': 3, 'kernel_size_2': 3, 'pool_size': 2, 'dense_activation1': 'relu', 'dense_activation2': 'sigmoid'}\n",
      "Total Run Time:  0:01:05.691883\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "index = 0\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "for combo in combinations:\n",
    "    index = index + 1\n",
    "    print()\n",
    "    print('--',index,'-of-',len(combinations),'-----------------------------------------------------------------------------------')\n",
    "    accuracies.append(train_neural_network(x_train[:100], y_train[:100], x_test[:100], y_test[:100], **combo))\n",
    "    \n",
    "end_time = datetime.datetime.now()    \n",
    "\n",
    "# Overwrite index\n",
    "index = np.argmax(accuracies)\n",
    "print('Index -', index)\n",
    "print('Value -', accuracies[index])\n",
    "print('Hyper-parameters', combinations[index])\n",
    "print('Total Run Time: ', str(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
