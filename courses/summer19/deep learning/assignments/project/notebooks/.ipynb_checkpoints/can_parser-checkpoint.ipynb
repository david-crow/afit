{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CAN data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV file 1...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'/Users/david/Library/CloudStorage/iCloud Drive/Documents/masters/thesis/data/ornl-data/can_data.csv' does not exist: b'/Users/david/Library/CloudStorage/iCloud Drive/Documents/masters/thesis/data/ornl-data/can_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c1a810081b47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     ornl_data = pd.read_csv(os.path.join(data_directory, \"ornl-data/can_data.csv\"),\n\u001b[1;32m      9\u001b[0m         dtype={\"captureid\": int, \"vehicle_id\": int, \"ts\": float, \"arbitration_id\": object, \n\u001b[0;32m---> 10\u001b[0;31m                \"dlc\": int, \"data\": object, \"make\": object, \"model\": object, \"year\": int})\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Reading CSV file 2...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'/Users/david/Library/CloudStorage/iCloud Drive/Documents/masters/thesis/data/ornl-data/can_data.csv' does not exist: b'/Users/david/Library/CloudStorage/iCloud Drive/Documents/masters/thesis/data/ornl-data/can_data.csv'"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "data = None\n",
    "\n",
    "data_directory = \"/Users/david/Library/CloudStorage/iCloud Drive/Documents/masters/thesis/data\"\n",
    "\n",
    "if not os.path.exists(os.path.join(data_directory, \"all_data.csv\")):\n",
    "    print(\"Reading CSV file 1...\")\n",
    "    ornl_data = pd.read_csv(os.path.join(data_directory, \"ornl-data\", \"can_data.csv\"),\n",
    "        dtype={\"captureid\": int, \"vehicle_id\": int, \"ts\": float, \"arbitration_id\": object, \n",
    "               \"dlc\": int, \"data\": object, \"make\": object, \"model\": object, \"year\": int})\n",
    "\n",
    "    print(\"Reading CSV file 2...\")\n",
    "    stone_data = pd.read_csv(os.path.join(data_directory, \"stone-data\", \"can_data.csv\"),\n",
    "        dtype={\"captureid\": int, \"vehicle_id\": int, \"Titsme\": float, \"arbitration_id\": object, \n",
    "               \"dlc\": int, \"data\": object, \"make\": object, \"model\": object, \"year\": int})\n",
    "\n",
    "    # rename the columns\n",
    "    ornl_data.columns = [\"Capture\", \"Vehicle\", \"Time\", \"ArbID\", \"DLC\", \"Data\", \"Make\", \"Model\", \"Year\"]\n",
    "    stone_data.columns = [\"Capture\", \"Vehicle\", \"Time\", \"ArbID\", \"DLC\", \"Data\", \"Make\", \"Model\", \"Year\"]\n",
    "    \n",
    "    # combine the dataframes\n",
    "    print(\"Combining dataframes...\")\n",
    "    data = pd.concat([ornl_data, stone_data])\n",
    "    data = data.reset_index(drop=True)\n",
    "    \n",
    "    print(\"Writing to \" + os.path.join(data_directory, \"all_data.csv\"))\n",
    "    data.to_csv(os.path.join(data_directory, \"all_data.csv\"), index=False)\n",
    "\n",
    "else:\n",
    "    print(\"File already exists!\")\n",
    "    data = pd.read_csv(os.path.join(data_directory, \"all_data.csv\"),\n",
    "        dtype={\"Capture\": int, \"Vehicle\": int, \"Time\": float, \"ArbID\": object, \n",
    "               \"DLC\": int, \"Data\": object, \"Make\": object, \"Model\": object, \"Year\": int})\n",
    "\n",
    "display(data.head(5))\n",
    "end = time.time()\n",
    "print(\"Elapsed time: {:.2f} seconds\".format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a metadata table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 179.50 seconds\n"
     ]
    }
   ],
   "source": [
    "# how long does this take?\n",
    "start = time.time()\n",
    "\n",
    "# number of bytes in each snapshot\n",
    "chunk_size = 1024\n",
    "\n",
    "# minimum number of snapshots for each arbID\n",
    "min_snaps = 1\n",
    "\n",
    "# used to build a dataframe\n",
    "captures = []\n",
    "vehicles = []\n",
    "arbIDs = []\n",
    "num_bytes = []\n",
    "num_snaps = []\n",
    "valid = []\n",
    "\n",
    "# for each capture...\n",
    "for i in np.sort(data[\"Capture\"].unique()):\n",
    "    capture = data[data[\"Capture\"] == i]\n",
    "    vehicle = capture[\"Vehicle\"].iloc[0]\n",
    "    \n",
    "    # for each arbID for capture i...\n",
    "    for j in capture[\"ArbID\"].unique():\n",
    "        arbID = capture[capture[\"ArbID\"] == j]\n",
    "        bytes_count = np.sum(arbID[\"DLC\"])\n",
    "        \n",
    "        captures.append(i)\n",
    "        vehicles.append(vehicle)\n",
    "        arbIDs.append(j)\n",
    "        num_bytes.append(bytes_count)\n",
    "        num_snaps.append(bytes_count // chunk_size)\n",
    "                \n",
    "        # every vehicle/arbID combo needs at least one picture\n",
    "        valid.append(\"yes\" if bytes_count >= chunk_size * min_snaps else \"no\")\n",
    "\n",
    "# build a dataframe\n",
    "metadata = pd.DataFrame(\n",
    "    np.hstack((\n",
    "        np.array(vehicles).reshape(-1,1),\n",
    "        np.array(captures).reshape(-1,1),\n",
    "        np.array(arbIDs).reshape(-1,1),\n",
    "        np.array(num_bytes).reshape(-1,1),\n",
    "        np.array(num_snaps).reshape(-1,1),\n",
    "        np.array(valid).reshape(-1,1)\n",
    "    )),\n",
    "    \n",
    "    columns = [\"Vehicle\", \"Capture\", \"ArbID\", \"Bytes\", \"Snapshots\", \"Valid?\"]\n",
    ")\n",
    "\n",
    "# update the datatypes\n",
    "for col in metadata.columns.drop([\"ArbID\", \"Valid?\"]):\n",
    "    metadata[col] = pd.to_numeric(metadata[col])\n",
    "\n",
    "print(\"Elapsed time: {:.2f} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vehicle</th>\n",
       "      <th>Capture</th>\n",
       "      <th>ArbID</th>\n",
       "      <th>Bytes</th>\n",
       "      <th>Snapshots</th>\n",
       "      <th>Valid?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1217</td>\n",
       "      <td>7464</td>\n",
       "      <td>7</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>440192</td>\n",
       "      <td>429</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2024</td>\n",
       "      <td>440192</td>\n",
       "      <td>429</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>705</td>\n",
       "      <td>214256</td>\n",
       "      <td>209</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>708</td>\n",
       "      <td>288792</td>\n",
       "      <td>282</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Vehicle  Capture ArbID   Bytes  Snapshots Valid?\n",
       "0        1        1  1217    7464          7    yes\n",
       "1        1        1  2015  440192        429    yes\n",
       "2        1        1  2024  440192        429    yes\n",
       "3        1        1   705  214256        209    yes\n",
       "4        1        1   708  288792        282    yes"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1012 valid ArbIDs from 20 vehicles\n",
      "164 invalid ArbIDs from 12 vehicles\n",
      "297253 snapshots\n"
     ]
    }
   ],
   "source": [
    "# sort and show the dataframe\n",
    "metadata = metadata.sort_values(by=[\"Vehicle\", \"Capture\", \"ArbID\"]).reset_index(drop=True)\n",
    "display(metadata.head(5))\n",
    "\n",
    "valid = metadata[metadata[\"Valid?\"] == \"yes\"]\n",
    "invalid = metadata[metadata[\"Valid?\"] == \"no\"]\n",
    "\n",
    "valid_arbs = 0\n",
    "for i in valid[\"Vehicle\"].unique():\n",
    "    valid_arbs += len(valid[valid[\"Vehicle\"] == i][\"ArbID\"].unique())\n",
    "\n",
    "invalid_arbs = 0\n",
    "for i in invalid[\"Vehicle\"].unique():\n",
    "    invalid_arbs += len(invalid[invalid[\"Vehicle\"] == i][\"ArbID\"].unique())\n",
    "\n",
    "print(\"{} valid ArbIDs from {} vehicles\".format(\n",
    "    valid_arbs,\n",
    "    len(valid[\"Vehicle\"].unique())\n",
    "))\n",
    "\n",
    "print(\"{} invalid ArbIDs from {} vehicles\".format(\n",
    "    invalid_arbs,\n",
    "    len(invalid[\"Vehicle\"].unique())\n",
    "))\n",
    "\n",
    "print(\"{} snapshots\".format(np.sum(metadata[metadata[\"Valid?\"] == \"yes\"][\"Snapshots\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 & 8 & 4440 & 1.49 \\% \\\\\n",
      "2 & 47 & 6895 & 2.32 \\% \\\\\n",
      "3 & 49 & 141847 & 47.72 \\% \\\\\n",
      "4 & 103 & 14633 & 4.92 \\% \\\\\n",
      "5 & 112 & 43377 & 14.59 \\% \\\\\n",
      "6 & 79 & 9511 & 3.20 \\% \\\\\n",
      "7 & 124 & 35142 & 11.82 \\% \\\\\n",
      "8 & 35 & 5018 & 1.69 \\% \\\\\n",
      "9 & 21 & 8211 & 2.76 \\% \\\\\n",
      "101 & 29 & 4102 & 1.38 \\% \\\\\n",
      "102 & 50 & 1824 & 0.61 \\% \\\\\n",
      "103 & 62 & 1757 & 0.59 \\% \\\\\n",
      "104 & 78 & 2182 & 0.73 \\% \\\\\n",
      "105 & 24 & 3791 & 1.28 \\% \\\\\n",
      "106 & 28 & 1695 & 0.57 \\% \\\\\n",
      "107 & 42 & 2198 & 0.74 \\% \\\\\n",
      "108 & 38 & 3020 & 1.02 \\% \\\\\n",
      "109 & 26 & 2553 & 0.86 \\% \\\\\n",
      "110 & 19 & 2974 & 1.00 \\% \\\\\n",
      "111 & 38 & 2083 & 0.70 \\% \\\\\n"
     ]
    }
   ],
   "source": [
    "total_snaps = np.sum(valid[\"Snapshots\"])\n",
    "\n",
    "for i in valid[\"Vehicle\"].unique():\n",
    "    temp = valid[valid[\"Vehicle\"] == i]\n",
    "    num_arbs = len(temp[\"ArbID\"].unique())\n",
    "    num_snaps = np.sum(temp[\"Snapshots\"])\n",
    "    \n",
    "    print(\"{} & {} & {} & {:.2f} \\% \\\\\\\\\".format(i, num_arbs, num_snaps, 100. * num_snaps / total_snaps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the snapshots to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Capture'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2656\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2657\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Capture'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-d3122410590f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# for every capture (each is tied to a vehicle)...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapture\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Capture\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting capture {} ({}/{})...\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcapture\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Capture\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mcapture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Capture\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcapture\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2925\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2926\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2927\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2929\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2657\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2659\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2660\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Capture'"
     ]
    }
   ],
   "source": [
    "filename = \"../snapshots/snapshots.csv\"\n",
    "\n",
    "# delete any snapshot files that already exist\n",
    "# we do this because we open the file with append,\n",
    "# but we don't want to append to previously-created files\n",
    "if os.path.exists(filename):\n",
    "    os.remove(filename)\n",
    "\n",
    "# how long does this take?\n",
    "start = time.time()\n",
    "\n",
    "# make the big file\n",
    "f = open(filename, \"w+\")\n",
    "f.write(\"Vehicle,Capture,ArbID\")\n",
    "for i in range(1024):\n",
    "    f.write(\",Data \" + str(i + 1))\n",
    "f.write(\"\\n\")\n",
    "\n",
    "# we only want ArbIDs with at least <min_snaps> snapshots\n",
    "valid = metadata[metadata[\"Valid?\"] == \"yes\"]\n",
    "\n",
    "# for every capture (each is tied to a vehicle)...\n",
    "for count, capture in enumerate(valid[\"Capture\"].unique()):\n",
    "    print(\"Starting capture {} ({}/{})...\".format(capture, count + 1, len(valid[\"Capture\"].unique())))\n",
    "    capture = valid[valid[\"Capture\"] == capture]\n",
    "    vehicle = capture[\"Vehicle\"].iloc[0]\n",
    "    \n",
    "    # for every arbID in this capture...\n",
    "    for _, row in capture.iterrows():\n",
    "        # we only care about one arbID from one capture of one vehicle\n",
    "        can = data[data[\"Capture\"] == row[\"Capture\"]]\n",
    "        can = can[can[\"ArbID\"] == row[\"ArbID\"]]\n",
    "        \n",
    "        # join all hex data values into one big string\n",
    "        can_data = can[\"Data\"].str.cat(sep=\"\")\n",
    "        \n",
    "        # split big string into array of hex bytes\n",
    "        can_data = [can_data[i:i+2] for i in range(0, len(can_data), 2)]\n",
    "        \n",
    "        # convert each hex byte into a three-digit integer\n",
    "        for i, d in enumerate(can_data):\n",
    "            can_data[i] = str(int(d, 16)).zfill(3)\n",
    "        \n",
    "        # write every set of <chunk_size> bytes to the file\n",
    "        for i in range(0, len(can_data) - chunk_size, chunk_size):\n",
    "            f.write(\"{},{},{},{}\\n\".format(\n",
    "                row[\"Vehicle\"],\n",
    "                row[\"Capture\"],\n",
    "                row[\"ArbID\"],\n",
    "                \",\".join(can_data[i:i+chunk_size])\n",
    "            ))\n",
    "\n",
    "f.close()\n",
    "elapsed = time.time() - start\n",
    "print(\"Elapsed time: {:.2f} seconds ({:.2f} minutes)\".format(elapsed, elapsed / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf] *",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
