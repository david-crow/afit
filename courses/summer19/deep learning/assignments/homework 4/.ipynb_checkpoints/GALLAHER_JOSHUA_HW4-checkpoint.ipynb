{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "# Original network adapted from karpathy\n",
    "# minesh.mathew@gmail.com\n",
    "# modified version of text generation example in keras;\n",
    "# trained in a many-to-many fashion using a time distributed dense layer\n",
    "\n",
    "####\n",
    "from __future__ import print_function\n",
    "\n",
    "import gc\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "import matplotlib as mptl\n",
    "import pylab\n",
    "from collections import defaultdict\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "# from keras.layers import LSTM, TimeDistributedDense, SimpleRNN  #DEPRECATED TimeDistributedDense\n",
    "from keras.layers import LSTM, TimeDistributed, SimpleRNN, Embedding, GRU, Bidirectional, CuDNNLSTM\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import gzip\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "#import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- GENERIC UTILITIES --------------------------\n",
    "def plot_confusion_matrix(cm, class_labels, title='Confusion matrix',\n",
    "                          filename = 'Confusion_Matrix.png', cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    Plot a confusion matrix\n",
    "    :param cm: a confusion matrix generated by sklearn.metrics.confusion_matrix\n",
    "    :param class_labels: set of text labels\n",
    "    :param title: title of figure\n",
    "    :param cmap: color map for the confusion matrix\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    class_count = len(class_labels)\n",
    "    fig = plt.figure(title)\n",
    "    fig.set_size_inches(10, 8)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(class_count+1)\n",
    "    plt.xticks(tick_marks, class_labels, rotation=45)\n",
    "    plt.yticks(tick_marks, class_labels)\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    #plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig(filename, dpi=100)\n",
    "    np.set_printoptions(precision=2)\n",
    "    plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------- TEXT FILE PREPROCESSING -----------------\n",
    "\n",
    "def preprocess_text_file(filename, classID, maxlen=40 ):\n",
    "    \"\"\"load a file and split the text it contains into sequences of length = maxlen\n",
    "    returns text, chars, char_indices, indices_char, sentences, next_chars\n",
    "    text: the raw text (turned into lowercase)\n",
    "    chars: a list of unique characters in the text\n",
    "    char_indices: a dictionary of the character-to-index conversion\n",
    "    indices_char: a dictionary of the index-to-character conversion\n",
    "    sequences: a list of the sequences of max length extracted from the file (stride specified by the step variable below)\n",
    "    next_chars: a list of the corresponding sequences of max_length next-characters following each of the sequence character members\n",
    "      in other words, each member of next_chars is contains the last maxlen-1 chars of the correspondiong sequence ...\n",
    "      and the next character from the text after the last character in that sequence \"\"\"\n",
    "# #     print('loading: ', filename)\n",
    "    text = open(filename).read().lower()\n",
    "    print('corpus length:', len(text))\n",
    "    chars = sorted(list(set(text)))  #returns unique characters from the text\n",
    "#     print('total chars:', len(chars))\n",
    "    char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "#     print('chars_indices:', char_indices)\n",
    "    indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "    step = 1\n",
    "    length = 400\n",
    "    sequences = []\n",
    "    classIDs = []\n",
    "    \n",
    "    #first generate sentences of characters\n",
    "    for i in range(0, len(text) - length + 1, length):\n",
    "        sequences.append(text[i: i + length])  # input seq is from i to i  + maxlen\n",
    "\n",
    "    # Get the train, val, and test sets\n",
    "    \n",
    "    smoltrainX = []\n",
    "    smolvalX = []\n",
    "    smoltestX = []\n",
    "    \n",
    "    bigTrainX = []\n",
    "    bigValX = []\n",
    "    bigTestX = []\n",
    "     \n",
    "    trainY = []\n",
    "    valY = []\n",
    "    testY = []\n",
    "    \n",
    "    # Parse the sentences into the training, val, and test sets\n",
    "    for i in range(len(sequences)):\n",
    "        smoltrainX.append( sequences[i][ 0 : 159 ] )\n",
    "        smolvalX.append( sequences[i][ 160 : 319 ] )\n",
    "        smoltestX.append( sequences[i][ 320 : ] )\n",
    "    \n",
    "    \n",
    "    # Create the 40 char length for each set\n",
    "    for sentence in smoltrainX:\n",
    "        for i in range( 0, len(sentence ) - maxlen + 1, step):\n",
    "            bigTrainX.append( sentence[ i : i + maxlen ] )\n",
    "            trainY.append( classID )\n",
    "         \n",
    "    for sentence in smolvalX:\n",
    "        for i in range( 0, len(sentence ) - maxlen + 1, step):\n",
    "            bigValX.append( sentence[ i : i + maxlen ] )\n",
    "            valY.append( classID )\n",
    "        \n",
    "    for sentence in smoltestX:\n",
    "        for i in range( 0, len(sentence ) - maxlen + 1, step):\n",
    "            bigTestX.append( sentence[ i : i + maxlen ] )\n",
    "            testY.append( classID )\n",
    "    \n",
    "    return text, chars, char_indices, indices_char, sequences, bigTrainX, trainY, bigValX, valY, bigTestX, testY\n",
    "\n",
    "\n",
    "\n",
    "def save_processed_text(filename, text, chars, char_indices, indices_char, sentences, next_chars ):\n",
    "    \"\"\"helper method for saving processed text in a compressed file so that you dont have to preprocess it each time\"\"\"\n",
    "    my_text = [text, chars, char_indices, indices_char, sentences, next_chars] #dont save the one-hot vectors\n",
    "    with gzip.open(filename, 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "        pickle.dump(my_text, f)\n",
    "    # np.savez_compressed(filename,\n",
    "    #                     text = text,\n",
    "    #                     chars = chars,\n",
    "    #                     char_indices=char_indices,\n",
    "    #                     indices_char=indices_char,\n",
    "    #                     sentences=sentences,\n",
    "    #                     next_chars=next_chars)\n",
    "    #                     # allow_pickle=True, protocol=4)\n",
    "\n",
    "\n",
    "def load_processed_text(filename):\n",
    "    \"\"\"helper method for saving processed text in a compressed file\"\"\"\n",
    "    with gzip.open(filename, 'rb') as f:  # Python 3: open(..., 'rb')\n",
    "        text, chars, char_indices, indices_char, sentences, next_chars = pickle.load(f)\n",
    "    # loaded = np.load(filename)\n",
    "    # text = loaded['text']\n",
    "    # chars = loaded['chars']\n",
    "    # char_indices= loaded['char_indices']\n",
    "    # indices_char= loaded['indices_char']\n",
    "    # sentences= loaded['sentences']\n",
    "    # next_chars= loaded['next_chars']\n",
    "    return text, chars, char_indices, indices_char, sentences, next_chars\n",
    "\n",
    "\n",
    "def vectorize_text(chars, char_indices, sentences, label, maxlen=40):  #UNUSED: \"next_chars\"\n",
    "    \"\"\"Accepts a list of sentences to convert to indices.  Used characters, their corresponding indeces to produce a set of sequences\n",
    "    of X and corresponding labels y\"\"\"\n",
    "    # now generate dummy variables (1-hot vectors) for the sequences of characters\n",
    "    print('Vectorization processing... this could take a while...')\n",
    "    print(len(sentences))\n",
    "    print(len(chars))\n",
    "    X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "#     y = np.zeros((len(sentences), maxlen, len(chars)),\n",
    "#                  dtype=np.bool)  # y is also a sequence , or  a seq of 1 hot vectors\n",
    "    joblength = len(sentences)\n",
    "    tenpercent = joblength/10\n",
    "    nextpercent = tenpercent\n",
    "    print(\" part 1 of 2\")\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        if i>nextpercent:\n",
    "            print(i, \" of \", joblength, \" completed\")\n",
    "            nextpercent += tenpercent\n",
    "        for t, char in enumerate(sentence):\n",
    "            X[i, t, char_indices[char]] = 1  # X has dimension [sentence_count, sentence_length, char_count]\n",
    "    print(\" part 2 of 2\")\n",
    "    nextpercent = tenpercent\n",
    "\n",
    "#     y = to_categorical( label )\n",
    "    \n",
    "    print('vetorization completed')\n",
    "    return X\n",
    "\n",
    "\n",
    "def generate_text(model, char_indices, indices_char, seed_string=\"brutus:\", generate_character_count=320):\n",
    "    \"\"\"Generates text using a model\"\"\"\n",
    "    print(\"seed string --> \", seed_string)\n",
    "    print('The generated text is: ')\n",
    "    sys.stdout.write(seed_string),\n",
    "    # x=np.zeros((1, len(seed_string), len(chars)))\n",
    "    for i in range(generate_character_count):\n",
    "        x = np.zeros((1, len(seed_string), len(chars)))\n",
    "        for t, char in enumerate(seed_string):\n",
    "            x[0, t, char_indices[char]] = 1.\n",
    "        preds = model.predict(x, verbose=0)[0]\n",
    "        # print (np.argmax(preds[7]))\n",
    "        next_index = np.argmax(preds[len(seed_string) - 1])\n",
    "\n",
    "        # next_index=np.argmax(preds[len(seed_string)-11])\n",
    "        # print (preds.shape)\n",
    "        # print (preds)\n",
    "        # next_index = sample(preds, 1) #diversity is 1\n",
    "        next_char = indices_char[next_index]\n",
    "        seed_string = seed_string + next_char\n",
    "\n",
    "        # print (seed_string)\n",
    "        # print ('##############')\n",
    "        # if i==40:\n",
    "        #    print ('####')\n",
    "        sys.stdout.write(next_char)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- MODEL FILE I/O ---------------------------\n",
    "def save_model(model, save_dir=os.path.join(os.getcwd(), 'saved_models'),\n",
    "               model_file_name='keras_cifar10_trained_model.h5'):\n",
    "    \"\"\"\n",
    "    Save model and current weights\n",
    "    :param model: Keras model\n",
    "    :param save_dir: path name to save directory\n",
    "    :param model_file_name: filename for saved model\n",
    "    :return: nothing\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    model_path = os.path.join(save_dir, model_file_name)\n",
    "    model.save(model_path)\n",
    "    print('Saved trained model at %s ' % model_path)\n",
    "\n",
    "\n",
    "def load_model(save_dir, model_file_name):\n",
    "    # Load model and weights\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    model_path = os.path.join(save_dir, model_file_name)\n",
    "    model = keras.models.load_model(model_path)\n",
    "    print('Loaded trained model from %s ' % model_path)\n",
    "    return model\n",
    "\n",
    "\n",
    "# ---------------- MODEL ARCHITECTURE ---------------------------\n",
    "def build_model(characters):\n",
    "    # build the model: 2 stacked LSTM\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "    # model.add(LSTM(512, return_sequences=True, input_shape=(maxlen, len(chars))))  # original one\n",
    "    #model.add(LSTM(512, input_dim=len(characters), return_sequences=True))  # minesh witout specifying the input_length\n",
    "    model.add(LSTM(512, return_sequences=True, input_shape=(None,len(characters)) ))  # minesh witout specifying the input_length\n",
    "    model.add(LSTM(512, return_sequences=True))  # - original\n",
    "    model.add(Dropout(0.2))\n",
    "    # model.add(TimeDistributedDense(len(chars)))   #Deprecated TimeDistributedDense\n",
    "    model.add(TimeDistributed(Dense(len(characters))))  # BJB:  is this really working??\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "    print('model is made')\n",
    "    # train the model, output generated text after each iteration\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "# PROBLEMS TO FIX...\n",
    "# C:\\Users\\bborghetti\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:34: \n",
    "#             UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
    "# C:\\Users\\bborghetti\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:34: \n",
    "#             UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(512, return_sequences=True, input_shape=(None, 45))`\n",
    "\n",
    "\n",
    "\n",
    "# ---------------- MODEL TRAINING ---------------------------\n",
    "# def train_net(model, x, y, training_iterations=6, maxlen=40, save_all_model_iterations=True):\n",
    "#     chkRNN = ModelCheckpoint('best_modelRNN.h5', monitor='val_acc', save_best_only=True, mode='max', verbose=1)\n",
    "#     for training_iteration in range(1, training_iterations+1):\n",
    "#         print()\n",
    "#         print('-' * 50)\n",
    "#         print('Training Iteration (epoch) #:', training_iteration)\n",
    "#         history = model.fit(x, \n",
    "#                             y, \n",
    "#                             batch_size=128, \n",
    "#                             epochs=1, \n",
    "#                             verbose=1)    #train 1 epoch at a time using previous weights\n",
    "#         sleep(0.1)  # https://github.com/fchollet/keras/issues/2110\n",
    "\n",
    "#         # saving models at the following iterations -- uncomment it if you want tos save weights and load it later\n",
    "#         # if training_iteration==1 or training_iteration==3 or training_iteration==5 or training_iteration==10 or training_iteration==20 or training_iteration==30 or training_iteration==50 or training_iteration==60 :\n",
    "\n",
    "#         # # save every training_iteration of weights\n",
    "#         # model.save_weights('Karpathy_LSTM_weights_' + str(training_iteration) + '.h5', overwrite=True)\n",
    "#         # start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "        \n",
    "#         save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "#         current_model_file_name = 'LSTM_model_' + str(training_iteration) + '.h5'\n",
    "#         if save_all_model_iterations:\n",
    "#             save_model(model=model, save_dir=save_dir, model_file_name=current_model_file_name)\n",
    "#         sys.stdout.flush()\n",
    "#         print('Training:\\n\\tLoss: {}\\n\\tAccuracy: {}'.format( history.history['loss'][0], history.history['acc'][0]) )\n",
    "#         print()\n",
    "#         print('Validation:\\n\\tLoss: {}\\n\\tAccuracy: {}'.format( history.history['val_loss'][0], history.history['val_acc'][0]) )\n",
    "#         print()\n",
    "#     return model\n",
    "\n",
    "\n",
    "# ---------------- MODEL EVALUATION ---------------------------\n",
    "# def test_model(model, observations, targets):\n",
    "#     '''\n",
    "#     STUDENT SHOULD WRITE THIS CODE\n",
    "#     :param model: a trained RNN model which accepts a sequence and outputs a target class (0;1;2;3)\n",
    "#     :param observations: a list of 40-character sequences to classify\n",
    "#     :param targets: a list of the true classes of the 40-character sequences\n",
    "#     :return: a sklearn confusion matrix\n",
    "#     '''\n",
    "#     #< put student code here to test the model >\n",
    "#     predicted_class_IDs = model.predict_classes( observations )\n",
    "#     actual_class_IDs = targets\n",
    "\n",
    "#     # generate & print confusion matrix to screen\n",
    "#     cm = confusion_matrix(actual_class_IDs, predicted_class_IDs)\n",
    "#     return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './textdatasets/1_nietzsche.txt.pklz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-8346c314b0b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[0msave_processed_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessed_filename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchar_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices_char\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_chars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# instead, load previously processed text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m     \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchar_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices_char\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_chars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_processed_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessed_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-e051ddb36b58>\u001b[0m in \u001b[0;36mload_processed_text\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_processed_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[1;34m\"\"\"helper method for saving processed text in a compressed file\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mgzip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Python 3: open(..., 'rb')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m         \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchar_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices_char\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_chars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[1;31m# loaded = np.load(filename)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\dl\\lib\\gzip.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(filename, mode, compresslevel, encoding, errors, newline)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mgz_mode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"t\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m         \u001b[0mbinary_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGzipFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgz_mode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"read\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"write\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[0mbinary_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGzipFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgz_mode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\dl\\lib\\gzip.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[0;32m    161\u001b[0m             \u001b[0mmode\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m'b'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfileobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m             \u001b[0mfileobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmyfileobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'name'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './textdatasets/1_nietzsche.txt.pklz'"
     ]
    }
   ],
   "source": [
    "################################################################\n",
    "#--------------------- Main Code -----------------------------\n",
    "\n",
    "\n",
    "# pick the filename you want to use, and comment out the rest\n",
    "# make sure you have this directory structure\n",
    "# QUICK CHECK w/SMALL TEXT\n",
    "# raw_text_filename='./textdatasets/tinytesttext.txt'\n",
    "# CLASS DATASETS BELOW\n",
    "# raw_text_filename='./textdatasets/0_bible.txt'\n",
    "raw_text_filename='./textdatasets/1_nietzsche.txt'\n",
    "# raw_text_filename='./textdatasets/2_shakespeare.txt'\n",
    "# raw_text_filename='./textdatasets/3_warpeace.txt'\n",
    "\n",
    "\n",
    "\n",
    "#raw_text_filename='./textdatasets/trumptweets.txt'\n",
    "\n",
    "processed_filename = raw_text_filename+'.pklz'  # save process will append a .pklz on the filename\n",
    "\n",
    "#set the boolean below to true to double check the save file is working\n",
    "TEST_SAVE_LOAD_EQUAL = False\n",
    "if TEST_SAVE_LOAD_EQUAL:\n",
    "    text, chars, char_indices, indices_char, sentences, next_chars \\\n",
    "        = preprocess_text_file(raw_text_filename)\n",
    "    save_processed_text(processed_filename, text, chars, char_indices, indices_char, sentences, next_chars )\n",
    "    _text, _chars, _char_indices, _indices_char, _sentences, _next_chars = load_processed_text(processed_filename)\n",
    "    print(\"Testing equivalences of preprocessed and  saved-preprocessed text (6 checks)\")\n",
    "    print(\"  text equal: \",np.array_equal(_text, text))\n",
    "    print(\"  chars equal: \",np.array_equal(_chars, chars))\n",
    "    print(\"  char_indices equal: \",np.array_equal(_char_indices, char_indices))\n",
    "    print(\"  indices_chars equal: \",np.array_equal(_indices_char, indices_char))\n",
    "    print(\"  sentences equal: \",np.array_equal(_sentences, sentences))\n",
    "    print(\"  next_chars equal: \",np.array_equal(_next_chars, next_chars))\n",
    "    print(\"Testing vectorization\")\n",
    "    X, y = vectorize_text(_chars, _char_indices, _sentences, _next_chars)\n",
    "    print(\"Vectorization test complete\")\n",
    "\n",
    "    \n",
    "PROCESS_RAW_TEXT = False  #set to True to process a previously unseen textfile - otherwise load a preprocessed file\n",
    "if PROCESS_RAW_TEXT:\n",
    "    text, chars, char_indices, indices_char, sentences, next_chars \\\n",
    "        = preprocess_text_file(raw_text_filename, classID=0 ) \n",
    "    save_processed_text(processed_filename, text, chars, char_indices, indices_char, sentences, next_chars)\n",
    "else:  # instead, load previously processed text\n",
    "    text, chars, char_indices, indices_char, sentences, next_chars = load_processed_text(processed_filename)\n",
    "\n",
    "\n",
    "#vectorized form takes too much space to save... so process in real time\n",
    "X, y = vectorize_text(chars, char_indices, sentences, next_chars)\n",
    "\n",
    "\n",
    "TRAIN_MODE = False   #SET TO FALSE BEFORE SUBMITTING YOUR ASSIGNMENT!!!\n",
    "if TRAIN_MODE:\n",
    "    model = build_model(characters=chars)\n",
    "    model_epoch_training_iterations = 10    #the bigger your text corpus, the smaller you can make this\n",
    "    model = train_net(model=model, x=X, y=y,\n",
    "                      training_iterations=model_epoch_training_iterations,\n",
    "                      save_all_model_iterations=True)\n",
    "else:  # load a model from a file\n",
    "    # decide which iteration of the trained model you want to explore\n",
    "    model_training_iteration = 8\n",
    "    save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "    current_model_file_name = 'LSTM_model_' + str(model_training_iteration) + '.h5'\n",
    "    model = load_model(save_dir=save_dir, model_file_name=current_model_file_name)\n",
    "\n",
    "GENERATE_TEXT_MODE = False\n",
    "if GENERATE_TEXT_MODE:   # generate text mode\n",
    "    #decide which saved model to load\n",
    "    #make up a string of characters to start with\n",
    "    seed_string = \"certainly\"\n",
    "    # decide how many text characters you want to generate:\n",
    "    gen_char_count = 200\n",
    "    generate_text(model, char_indices, indices_char, seed_string, generate_character_count=gen_char_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1\n",
    "\n",
    "Student Architecture Design: Your goal in this step is to design and document your proposed model.\n",
    "\n",
    "   a.Design the model:  Your model has a different ML task than Karpathy’s – you need to determine from which text a sequence of characters most likely came from.  Your input observation will be a string of text (40 characters) drawn from the training text.  Your output will be a class ID (0_bible.txt; 1_nietzsche.text; 2_shakespeare.txt; 3_warpeace.txt;).  You can provide a class ID for each character in the input sequence, but you must provide an overall class ID (0, 1, 2, or 3) for the whole input sequence.  You could use one-hot encoding here.\n",
    "\n",
    "   b.Document your design:  Examine the layers of your model in the code, and develop a diagram (e.g. using Keras functions and/or Powerpoint) of the layers like you’ve seen in class or on the internet.  In text, describe the architecture and its parameterization.  Discuss: How many parameters are there in your whole model?  In each layer?  How did you decide on overall size/capacity? What other features are present in your model (like dropout) and why did you use them?  How does your model differ from the Karpathy-like model? \n",
    "\n",
    "   c.Implement your design in code \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildMyModel():\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(CuDNNLSTM( 10, input_shape=( None, len(chars))))\n",
    "#     model.add( Bidirectional( CuDNNLSTM( 25, input_shape=( None, len(chars) ) ) ) )\n",
    "    \n",
    "    model.add(Dropout( 0.2 ) )\n",
    "    \n",
    "    # Because we are using one hot encoding to represent the class, we need to ouput 4 values, one for each class\n",
    "    model.add( Dense( 4, activation='softmax' ) ) \n",
    "    \n",
    "    # Use categorical cross entropy for a multiclass classification problem\n",
    "    model.compile( loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chars' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-96145cec0d0c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgallaher\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuildMyModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-ee832ca295da>\u001b[0m in \u001b[0;36mbuildMyModel\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCuDNNLSTM\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;31m#     model.add( Bidirectional( CuDNNLSTM( 25, input_shape=( None, len(chars) ) ) ) )\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'chars' is not defined"
     ]
    }
   ],
   "source": [
    "gallaher = buildMyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This model has not yet been built. Build the model first by calling build() or calling fit() with some data. Or specify input_shape or batch_input_shape in the first layer for automatic build. ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-0c16e73232e8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgallaher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\network.py\u001b[0m in \u001b[0;36msummary\u001b[1;34m(self, line_length, positions, print_fn)\u001b[0m\n\u001b[0;32m   1250\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1251\u001b[0m             raise ValueError(\n\u001b[1;32m-> 1252\u001b[1;33m                 \u001b[1;34m'This model has not yet been built. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1253\u001b[0m                 \u001b[1;34m'Build the model first by calling build() '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1254\u001b[0m                 \u001b[1;34m'or calling fit() with some data. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: This model has not yet been built. Build the model first by calling build() or calling fit() with some data. Or specify input_shape or batch_input_shape in the first layer for automatic build. "
     ]
    }
   ],
   "source": [
    "gallaher.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.\tDesign customized code to generate training  / validation / test data:  \n",
    "Modify the existing training code to match your assigned task (inputs are character sequences and outputs are class IDs).  You will need to figure out how to read and vectorize each of the four text files and use them all to train your network.  You will also need to split the data into train (40%) / validation (40%) / test (20%) samples.   Do this is a way such that\n",
    "\n",
    "   •\tThe order of any characters in text is preserved, \n",
    "   \n",
    "   •\tThat none of the characters in any n-character sequence are used in more than one partition of the train/val/test split \n",
    "   \n",
    "   •\tThe text for each partition is interleaved so that all portions of the text are represented in each of the partitions (in other words, DON’T just cut the text by location in the file into beginning=training, middle = validation, end=test).  \n",
    "\n",
    "   •\tThe same amount of data from each class for training / validation.  Note that the text files for each class are different sizes, so be careful how you implement this.  You will need to use less than the full amount of text from each file.\n",
    "\n",
    "Assuming that the sequence length n=40, one way to do this split might be to take each chunk the text file into a sequence of 400-character segments (ignoring the final segment of less than 400 characters in the file) and then partition these segments into train, val, and test such that the first 160 chars are used for training, the second 160 chars are used for validation, and the last 80 chars are used for test.  Then, within each group, use vectorize_text to parse the text into sub-strings, and recombine the substrings into full sets for train; val; test sets.  Also, make sure that you do this segmentation with respect for the classes where the text came from.  A final caution:  don’t use off-the-shelf train-val-test splitters available in other packages unless you first ensure they meet all of the requirements expressed in this step… and document how they achieve this.  If you build your own code for this part, fully document how you did it in your report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the train, val, and test sets\n",
    "trainXHolder = []\n",
    "valXHolder = []\n",
    "testXHolder = []\n",
    "\n",
    "trainYHolder = []\n",
    "valYHolder = []\n",
    "testYHolder = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 4332559\n",
      "corpus length: 600901\n",
      "corpus length: 4573338\n",
      "corpus length: 3196232\n"
     ]
    }
   ],
   "source": [
    "PROCESS = True\n",
    "\n",
    "if PROCESS:\n",
    "    text0, chars0, char_indices0, indices_char0, sequences0, trX, trY, vX, vY, teX, teY = preprocess_text_file('./textdatasets/0_bible.txt', 0)\n",
    "    trainXHolder += ( trX ) \n",
    "    valXHolder   += ( vX )\n",
    "    testXHolder  += ( teX )\n",
    "    trainYHolder += ( trY )\n",
    "    valYHolder   += ( vY )\n",
    "    testYHolder  += ( teY )\n",
    "\n",
    "    text1, chars1, char_indices1, indices_char1, sequences1, trX, trY, vX, vY, teX, teY = preprocess_text_file('./textdatasets/1_nietzsche.txt', 1)\n",
    "    trainXHolder += ( trX )\n",
    "    valXHolder   += ( vX )\n",
    "    testXHolder  += ( teX )\n",
    "    trainYHolder += ( trY )\n",
    "    valYHolder   += ( vY )\n",
    "    testYHolder  += ( teY )\n",
    "\n",
    "    text2, chars2, char_indices2, indices_char2, sequences2, trX, trY, vX, vY, teX, teY = preprocess_text_file('./textdatasets/2_shakespeare.txt', 2)\n",
    "    trainXHolder += ( trX )\n",
    "    valXHolder   += ( vX )\n",
    "    testXHolder  += ( teX )\n",
    "    trainYHolder += ( trY )\n",
    "    valYHolder   += ( vY )\n",
    "    testYHolder  += ( teY )\n",
    "\n",
    "    text3, chars3, char_indices3, indices_char3, sequences3, trX, trY, vX, vY, teX, teY = preprocess_text_file('./textdatasets/3_warpeace.txt', 3)\n",
    "    trainXHolder += ( trX )\n",
    "    valXHolder   += ( vX )\n",
    "    testXHolder  += ( teX )\n",
    "    trainYHolder += ( trY )\n",
    "    valYHolder   += ( vY )\n",
    "    testYHolder  += ( teY )\n",
    "\n",
    "    # X, y = vectorize_text( chars, char_indices, trX, trY )\n",
    "\n",
    "    # create a dict that holds all chars and indices and free unneeded memory\n",
    "    text = text0 + text1 + text2 + text3\n",
    "    del text0, text1, text2, text3\n",
    "    chars = set( chars0 + chars1 + chars2 + chars3 )\n",
    "    del chars0, chars1, chars2, chars3\n",
    "    char_indices = {**char_indices0, **char_indices1, **char_indices2, **char_indices3}\n",
    "    del char_indices0, char_indices1, char_indices2, char_indices3\n",
    "    indices_char = {**indices_char0, **indices_char1, **indices_char2, **indices_char3}\n",
    "    del indices_char0, indices_char1, indices_char2, indices_char3\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3810720"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainXHolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization processing... this could take a while...\n",
      "3810720\n",
      "68\n",
      " part 1 of 2\n",
      "381073  of  3810720  completed\n",
      "762145  of  3810720  completed\n",
      "1143217  of  3810720  completed\n",
      "1524289  of  3810720  completed\n",
      "1905361  of  3810720  completed\n",
      "2286433  of  3810720  completed\n",
      "2667505  of  3810720  completed\n",
      "3048577  of  3810720  completed\n",
      "3429649  of  3810720  completed\n",
      " part 2 of 2\n",
      "vetorization completed\n",
      "Vectorization processing... this could take a while...\n",
      "3810720\n",
      "68\n",
      " part 1 of 2\n",
      "381073  of  3810720  completed\n",
      "762145  of  3810720  completed\n",
      "1143217  of  3810720  completed\n",
      "1524289  of  3810720  completed\n",
      "1905361  of  3810720  completed\n",
      "2286433  of  3810720  completed\n",
      "2667505  of  3810720  completed\n",
      "3048577  of  3810720  completed\n",
      "3429649  of  3810720  completed\n",
      " part 2 of 2\n",
      "vetorization completed\n"
     ]
    }
   ],
   "source": [
    "# Vectorize all the sets\n",
    "VECTORIZE = True\n",
    "\n",
    "if VECTORIZE:\n",
    "    gc.collect()\n",
    "    \n",
    "    trainX = vectorize_text( chars, char_indices, trainXHolder, trainYHolder )\n",
    "    trainY = to_categorical( trainYHolder )\n",
    "\n",
    "    gc.collect()\n",
    "    \n",
    "    valX = vectorize_text( chars, char_indices, valXHolder, valYHolder )\n",
    "    valY = to_categorical( valYHolder )\n",
    "\n",
    "    gc.collect()\n",
    "    \n",
    "#     testX = vectorize_text( chars, char_indices, testXHolder, testYHolder )\n",
    "#     testY = to_categorical( testYHolder )\n",
    "    \n",
    "    del trainXHolder, trainYHolder\n",
    "    del valXHolder, valYHolder\n",
    "#     del testXHolder, testYHolder\n",
    "    \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.\tDesign Validation-based Training Code:  \n",
    "\n",
    "Modify the provided training code to enable examining additional information per epoch such as training loss and accuracy and validation loss and accuracy.  You may want to implement early stopping based on the validation set, however since you will be training 1 epoch at a time, you could also just take performance measurements after each training epoch.  If you are making decisions using the validation data (i.e. early stopping), remember to not use the same data to also evaluate the model’s performance.  Include code to capture the validation performance so that it can be plotted (in a later HW step).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(model, trainx, trainy, valx, valy, training_iterations=6, maxlen=40, save_all_model_iterations=True):\n",
    "    chkRNN = ModelCheckpoint('best_modelRNN.h5', monitor='val_acc', save_best_only=True, mode='max', verbose=1)\n",
    "    for training_iteration in range(1, training_iterations+1):\n",
    "\n",
    "        print()\n",
    "        print('-' * 50)\n",
    "        print('Training Iteration (epoch) #:', training_iteration)\n",
    "        history = model.fit(trainx, \n",
    "                            trainy, \n",
    "                            batch_size=128, \n",
    "                            epochs=1, \n",
    "                            verbose=1,\n",
    "                            validation_data=( valx, valy ) )    #train 1 epoch at a time using previous weights\n",
    "        sleep(0.1)  # https://github.com/fchollet/keras/issues/2110\n",
    "\n",
    "        # saving models at the following iterations -- uncomment it if you want tos save weights and load it later\n",
    "        # if training_iteration==1 or training_iteration==3 or training_iteration==5 or training_iteration==10 or training_iteration==20 or training_iteration==30 or training_iteration==50 or training_iteration==60 :\n",
    "\n",
    "        # # save every training_iteration of weights\n",
    "        # model.save_weights('Karpathy_LSTM_weights_' + str(training_iteration) + '.h5', overwrite=True)\n",
    "        # start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "        \n",
    "        save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "        current_model_file_name = 'LSTM_model_' + str(training_iteration) + '.h5'\n",
    "        if save_all_model_iterations:\n",
    "            save_model(model=model, save_dir=save_dir, model_file_name=current_model_file_name)\n",
    "            # save the history too\n",
    "            history_name = 'model_history_' + str(training_iteration)\n",
    "            with open( history_name, 'wb' ) as handle:\n",
    "                pickle.dump( history.history, handle )\n",
    "                \n",
    "        sys.stdout.flush()\n",
    "        print('Training:\\n\\tLoss: {}\\n\\tAccuracy: {}'.format( history.history['loss'][0], history.history['acc'][0]) )\n",
    "        print()\n",
    "        print('Validation:\\n\\tLoss: {}\\n\\tAccuracy: {}'.format( history.history['val_loss'][0], history.history['val_acc'][0]) )\n",
    "        print()\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.\tDesign Testing code: \n",
    "The testing code provided in the HW is designed for the original Karpathy task.  Your testing code should have the signature test_model(model, observations, targets) and it should return a sklearn confusion matrix (cm).  observations is an arbitrary-sized list of 40-character sequences to classify, and targets is a list of the correct classes of those sequences.   A code shell for this function has been provided – but you will need to populate it with working code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, observations, targets):\n",
    "    '''\n",
    "    STUDENT SHOULD WRITE THIS CODE\n",
    "    :param model: a trained RNN model which accepts a sequence and outputs a target class (0;1;2;3)\n",
    "    :param observations: a list of 40-character sequences to classify\n",
    "    :param targets: a list of the true classes of the 40-character sequences\n",
    "    :return: a sklearn confusion matrix\n",
    "    '''\n",
    "    #< put student code here to test the model >\n",
    "    predicted_class_IDs = model.predict_classes( observations )\n",
    "    actual_class_IDs = targets\n",
    "\n",
    "    # generate & print confusion matrix to screen\n",
    "    cm = confusion_matrix(actual_class_IDs, predicted_class_IDs)\n",
    "    return cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.\tNew Architecture Training Check: \n",
    "Note that this classification task may not take as many training epochs as past HW assignments have taken – but each epoch may take longer to train than in previous assignments.  You may experience that overfitting begins earlier than in previous assignments.  Train your new Keras model for 2 epochs.  Confirm that training works (training loss lower in the second epoch) and track how long training takes.   You may need to adjust your model architecture based on the expected training time for 20 epochs (don’t build something that you don’t have time to train).  Report the estimated time for training 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------\n",
      "Training Iteration (epoch) #: 1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You are passing a target array of shape (1299720, 1) while using as loss `categorical_crossentropy`. `categorical_crossentropy` expects targets to be binary matrices (1s and 0s) of shape (samples, classes). If your targets are integer classes, you can convert them to the expected format via:\n```\nfrom keras.utils import to_categorical\ny_binary = to_categorical(y_int)\n```\n\nAlternatively, you can use the loss function `sparse_categorical_crossentropy` instead, which does expect integer targets.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-4d87b30eed03>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mmodel_epoch_training_iterations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m    \u001b[1;31m#the bigger your text corpus, the smaller you can make this\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     gallaher, history = train_net(model=gallaher, trainx=trainX, trainy=trainY, valx=valX, valy=valY,\n\u001b[1;32m---> 10\u001b[1;33m                       training_iterations=model_epoch_training_iterations)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-15-2bed4cb00225>\u001b[0m in \u001b[0;36mtrain_net\u001b[1;34m(model, trainx, trainy, valx, valy, training_iterations, maxlen, save_all_model_iterations)\u001b[0m\n\u001b[0;32m     11\u001b[0m                             \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m                             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m                             validation_data=( valx, valy ) )    #train 1 epoch at a time using previous weights\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# https://github.com/fchollet/keras/issues/2110\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m    953\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    807\u001b[0m                 \u001b[1;31m# using improper loss fns.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    808\u001b[0m                 check_loss_and_target_compatibility(\n\u001b[1;32m--> 809\u001b[1;33m                     y, self._feed_loss_fns, feed_output_shapes)\n\u001b[0m\u001b[0;32m    810\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    811\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mcheck_loss_and_target_compatibility\u001b[1;34m(targets, loss_fns, output_shapes)\u001b[0m\n\u001b[0;32m    271\u001b[0m                 raise ValueError(\n\u001b[0;32m    272\u001b[0m                     \u001b[1;34m'You are passing a target array of shape '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 273\u001b[1;33m                     \u001b[1;34m' while using as loss `categorical_crossentropy`. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    274\u001b[0m                     \u001b[1;34m'`categorical_crossentropy` expects '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m                     \u001b[1;34m'targets to be binary matrices (1s and 0s) '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: You are passing a target array of shape (1299720, 1) while using as loss `categorical_crossentropy`. `categorical_crossentropy` expects targets to be binary matrices (1s and 0s) of shape (samples, classes). If your targets are integer classes, you can convert them to the expected format via:\n```\nfrom keras.utils import to_categorical\ny_binary = to_categorical(y_int)\n```\n\nAlternatively, you can use the loss function `sparse_categorical_crossentropy` instead, which does expect integer targets."
     ]
    }
   ],
   "source": [
    "# model_training_iteration = 1\n",
    "#     save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "#     current_model_file_name = 'LSTM_model_' + str(model_training_iteration) + '.h5'\n",
    "#     gallaher = load_model(save_dir=save_dir, model_file_name=current_model_file_name)\n",
    "\n",
    "TRAIN_MODE = True   #SET TO FALSE BEFORE SUBMITTING YOUR ASSIGNMENT!!!\n",
    "if TRAIN_MODE:\n",
    "    model_epoch_training_iterations = 2    #the bigger your text corpus, the smaller you can make this\n",
    "    gallaher, history = train_net(model=gallaher, trainx=trainX, trainy=trainY, valx=valX, valy=valY,\n",
    "                      training_iterations=model_epoch_training_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphHistory( history, name ):\n",
    "\n",
    "    # Plotting the Accuracy vs Epoch Graph\n",
    "    plt.plot(history['acc'])\n",
    "    plt.plot(history['val_acc'])\n",
    "    plt.title(name + ' Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    # Plotting the Loss vs Epoch Graphs\n",
    "    plt.plot(history['loss'])\n",
    "    plt.plot(history['val_loss'])\n",
    "    plt.title( name + ' Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'History' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-438671adbb3b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgraphHistory\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mgallaher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'yee'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-45-df8cf0b66723>\u001b[0m in \u001b[0;36mgraphHistory\u001b[1;34m(history, name)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m# Plotting the Accuracy vs Epoch Graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' Model Accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'History' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "graphHistory( gallaher.history, 'yee')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.\tNew Architecture Training:  \n",
    "Train your network incrementally by epoch, monitoring both training and validation loss and accuracy.  Your goal is to design a network which could achieve a training accuracy of 100% and as high validation accuracy as possible.  Since you are saving each epoch’s model, you can backtrack to the model that achieves the highest validation accuracy for use in test.   In this way, you sidestep the danger of early stopping too early.  Plot training and validation curves as you go and use them to help you decide whether to stop training, keep training, or go back and redesign your network.\n",
    "\n",
    "   a. If your network training accuracy has not yet plateaued that means you should train for at least another epoch – maybe several more.\n",
    "\n",
    "   b.\tIf your network training accuracy plateaus for a few epochs and it is far below 100% accuracy and the loss is still improving, it may mean you just need to train it for more epochs (be patient).  \n",
    "    \n",
    "   c.\tIf your training loss plateaus then you may need to adjust the optimizer parameters such as learning rate or decay, AND/OR design a different network.  \n",
    "\n",
    "   d.\tCapacity check:  Once your training gets close to 100% with loss close to zero, then this means your network has sufficient capacity to learn the task and you are on your way to overfitting.  You are unlikely to be improving validation metrics if you keep training.  Note that dropout may disturb this type of capacity-check – dropout may prevent training performance from ever getting much better than validation performance.\n",
    "\n",
    "Your network still may not perform well in generalization (validation accuracy and loss) even if it has good training performance.  Don’t get too hung up on trying to get great validation accuracy – if you are above 50% accuracy for a 4 class classification problem, you are doing decently better than chance.  Provide plots of training and validation loss and accuracy and describe what happened in your decisionmaking process (especially if you decided to re-design your network).  Consult chapter 11 in the deep learning book for additional guidance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
